# prompt_logger.py - –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–∞ —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–æ–º—Ç–∞

import logging
import json
from datetime import datetime
from typing import List, Dict, Tuple, Optional

def log_step(step_num: int, title: str, content: str = "", success: bool = True):
    """–õ–æ–≥–∏—Ä—É–µ—Ç —à–∞–≥ –ø—Ä–æ—Ü–µ—Å—Å–∞ —Å –∫—Ä–∞—Å–∏–≤—ã–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    status = "‚úÖ" if success else "‚ùå"
    separator = "=" * 60
    
    print(f"\n{separator}")
    print(f"{status} –®–ê–ì {step_num}: {title}")
    print(f"{separator}")
    if content:
        print(content)

def log_chunk_info(chunks: list, chunk_type: str):
    """–õ–æ–≥–∏—Ä—É–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —á–∞–Ω–∫–∞—Ö"""
    print(f"\nüìä {chunk_type}: –Ω–∞–π–¥–µ–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤")
    for i, chunk in enumerate(chunks[:3]):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 3
        chunk_text = chunk.get('chunk_text', '')[:100]
        similarity = chunk.get('similarity', chunk.get('rank', 'N/A'))
        if isinstance(similarity, (int, float)):
            print(f"   {i+1}. [üéØ{similarity:.3f}] {chunk_text}...")
        else:
            print(f"   {i+1}. [üìä{similarity}] {chunk_text}...")
    if len(chunks) > 3:
        print(f"   ... –∏ –µ—â–µ {len(chunks) - 3} —á–∞–Ω–∫–æ–≤")

def filter_chunks_with_logging(chunks: list, chunk_type: str, exclude_doc_id=None, 
                             exclude_texts=None, limit=5) -> list:
    """–§–∏–ª—å—Ç—Ä—É–µ—Ç —á–∞–Ω–∫–∏ —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    print(f"\nüîç –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è {chunk_type}:")
    print(f"   üì• –í—Ö–æ–¥—è—â–∏—Ö —á–∞–Ω–∫–æ–≤: {len(chunks)}")
    
    filtered_texts = []
    excluded_by_doc = 0
    excluded_by_text = 0
    
    for chunk in chunks:
        chunk_text = chunk.get("chunk_text", "")
        metadata = chunk.get("metadata", {})
        
        # –§–∏–ª—å—Ç—Ä –ø–æ document_id
        if exclude_doc_id and str(metadata.get("document_id")) == str(exclude_doc_id):
            excluded_by_doc += 1
            continue
        # –§–∏–ª—å—Ç—Ä –ø–æ —Ç–µ–∫—Å—Ç—É
        if exclude_texts and chunk_text.strip() in exclude_texts:
            excluded_by_text += 1
            continue
            
        filtered_texts.append(chunk_text)
        if len(filtered_texts) >= limit:
            break
    
    print(f"   üö´ –ò—Å–∫–ª—é—á–µ–Ω–æ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç—É: {excluded_by_doc}")
    print(f"   üö´ –ò—Å–∫–ª—é—á–µ–Ω–æ –ø–æ —Ç–µ–∫—Å—Ç—É: {excluded_by_text}")
    print(f"   ‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã—Ö —á–∞–Ω–∫–æ–≤: {len(filtered_texts)}")
    
    return filtered_texts

async def process_user_question_detailed(user_id: int, user_input: str) -> Dict:
    """
    üîç –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø: –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å –ø–æ–¥—Ä–æ–±–Ω—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º
    
    Returns:
        Dict —Å –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ–º—Ç–∞
    """
    
    # ==========================================
    # –®–ê–ì 1: –ü–û–õ–£–ß–ï–ù–ò–ï –í–û–ü–†–û–°–ê –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø
    # ==========================================
    log_step(1, "–ü–û–õ–£–ß–ï–ù–ò–ï –í–û–ü–†–û–°–ê –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø", 
             f"üë§ –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å {user_id}\nüí¨ –í–æ–ø—Ä–æ—Å: '{user_input}'")
    
    try:
        # ==========================================
        # –®–ê–ì 2: –†–ê–°–®–ò–†–ï–ù–ò–ï –í–û–ü–†–û–°–ê –ß–ï–†–ï–ó GPT
        # ==========================================
        log_step(2, "–†–ê–°–®–ò–†–ï–ù–ò–ï –í–û–ü–†–û–°–ê –ß–ï–†–ï–ó GPT")
        
        try:
            from gpt import enrich_query_for_vector_search
            refined_query = await enrich_query_for_vector_search(user_input)
            
            print(f"üîç –ò—Å—Ö–æ–¥–Ω—ã–π –≤–æ–ø—Ä–æ—Å: '{user_input}'")
            print(f"üß† –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å: '{refined_query}'")
            print(f"üìè –î–ª–∏–Ω–∞: {len(user_input)} ‚Üí {len(refined_query)} —Å–∏–º–≤–æ–ª–æ–≤")
            
        except Exception as e:
            refined_query = user_input
            print(f"‚ùå –û—à–∏–±–∫–∞ GPT —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è: {e}")
            print(f"üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –≤–æ–ø—Ä–æ—Å: '{user_input}'")
        
        # ==========================================
        # –®–ê–ì 3: –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –ö–õ–Æ–ß–ï–í–´–• –°–õ–û–í
        # ==========================================
        log_step(3, "–ò–ó–í–õ–ï–ß–ï–ù–ò–ï –ö–õ–Æ–ß–ï–í–´–• –°–õ–û–í –ù–ê –ê–ù–ì–õ–ò–ô–°–ö–û–ú")
        
        try:
            from gpt import extract_keywords
            keywords = await extract_keywords(user_input)
            
            print(f"üîë –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞: {keywords}")
            print(f"üìä –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ: {len(keywords)}")
            
        except Exception as e:
            keywords = []
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {e}")
        
        # ==========================================
        # –®–ê–ì 4: –ü–û–î–ì–û–¢–û–í–ö–ê –ö –ü–û–ò–°–ö–£
        # ==========================================
        log_step(4, "–ü–û–î–ì–û–¢–û–í–ö–ê –ö –ü–û–ò–°–ö–£ –í –í–ï–ö–¢–û–†–ù–û–ô –ë–ê–ó–ï")
        
        # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        try:
            from db_postgresql import get_last_summary
            last_doc_id, last_summary = await get_last_summary(user_id)
            exclude_texts = last_summary.strip().split("\n\n") if last_summary else []
            
            print(f"üö´ –ò—Å–∫–ª—é—á–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç ID: {last_doc_id}")
            print(f"üö´ –ò—Å–∫–ª—é—á–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤ –∏–∑ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {len(exclude_texts)}")
            
        except Exception as e:
            last_doc_id, last_summary = None, ""
            exclude_texts = []
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è: {e}")
        
        # ==========================================
        # –®–ê–ì 5A: –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ò–ô –ü–û–ò–°–ö
        # ==========================================
        log_step(5, "–°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ò–ô –ü–û–ò–°–ö –ü–û –í–ï–ö–¢–û–†–ù–û–ô –ë–ê–ó–ï")
        
        try:
            from vector_db_postgresql import search_similar_chunks
            vector_chunks = await search_similar_chunks(user_id, refined_query, limit=10)
            
            if vector_chunks:
                log_chunk_info(vector_chunks, "–°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ò–ï –ß–ê–ù–ö–ò")
            else:
                print("‚ùå –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —á–∞–Ω–∫–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
                
        except Exception as e:
            vector_chunks = []
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")
        
        # ==========================================
        # –®–ê–ì 5B: –ü–û–ò–°–ö –ü–û –ö–õ–Æ–ß–ï–í–´–ú –°–õ–û–í–ê–ú
        # ==========================================
        log_step(6, "–ü–û–ò–°–ö –ü–û –ö–õ–Æ–ß–ï–í–´–ú –°–õ–û–í–ê–ú")
        
        try:
            from vector_db_postgresql import keyword_search_chunks
                       
            # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ø–µ—Ä–µ–¥–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞, –∞ –Ω–µ –∏—Å—Ö–æ–¥–Ω—ã–π –≤–æ–ø—Ä–æ—Å
            keywords_string = ", ".join(keywords) if keywords else user_input
            keyword_chunks = await keyword_search_chunks(user_id, keywords_string, limit=10)
            
            print(f"üîç –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º: '{keywords_string}'")
            
            if keyword_chunks:
                log_chunk_info(keyword_chunks, "–ö–õ–Æ–ß–ï–í–´–ï –ß–ê–ù–ö–ò")
            else:
                print("‚ùå –ß–∞–Ω–∫–æ–≤ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
                
        except Exception as e:
            keyword_chunks = []
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º: {e}")
        
        # ==========================================
        # –®–ê–ì 6: –§–ò–õ–¨–¢–†–ê–¶–ò–Ø –ò –û–ë–™–ï–î–ò–ù–ï–ù–ò–ï
        # ==========================================
        log_step(7, "–§–ò–õ–¨–¢–†–ê–¶–ò–Ø –ò –û–ë–™–ï–î–ò–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        vector_texts = filter_chunks_with_logging(
            vector_chunks, "–°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ò–•", 
            exclude_doc_id=last_doc_id, exclude_texts=exclude_texts, limit=4
        )
        
        keyword_texts = filter_chunks_with_logging(
            keyword_chunks, "–ö–õ–Æ–ß–ï–í–´–•", 
            exclude_doc_id=last_doc_id, exclude_texts=exclude_texts, limit=2
        )
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∏ —É–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        all_chunks = list(dict.fromkeys(vector_texts + keyword_texts))
        chunks_text = "\n\n".join(all_chunks[:6])
        
        print(f"\nüì¶ –ò–¢–û–ì–û–í–´–ô –†–ï–ó–£–õ–¨–¢–ê–¢ –ü–û–ò–°–ö–ê:")
        print(f"   üß† –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —á–∞–Ω–∫–æ–≤: {len(vector_texts)}")
        print(f"   üîë –ö–ª—é—á–µ–≤—ã—Ö —á–∞–Ω–∫–æ–≤: {len(keyword_texts)}")
        print(f"   üìã –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —á–∞–Ω–∫–æ–≤: {len(all_chunks)}")
        print(f"   üìÑ –°–∏–º–≤–æ–ª–æ–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {len(chunks_text)}")
        
        # ==========================================
        # –®–ê–ì 7: –°–ò–°–¢–ï–ú–ù–´–ô –ü–†–û–ú–¢
        # ==========================================
        log_step(8, "–°–û–ó–î–ê–ù–ò–ï –°–ò–°–¢–ï–ú–ù–û–ì–û –ü–†–û–ú–¢–ê")
        
        try:
            from db_postgresql import get_user_language
            lang = await get_user_language(user_id)
            
            system_prompt = (
                "You are a compassionate and knowledgeable virtual physician who guides the user through their medical journey. "
                "You speak in a friendly, human tone and provide explanations when needed. "
                f"Always respond in the '{lang}' language."
            )
            
            print(f"üåê –Ø–∑—ã–∫ –æ—Ç–≤–µ—Ç–∞: {lang}")
            print(f"üìè –î–ª–∏–Ω–∞ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º—Ç–∞: {len(system_prompt)} —Å–∏–º–≤–æ–ª–æ–≤")
            
        except Exception as e:
            system_prompt = "You are a helpful medical assistant."
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º—Ç–∞: {e}")
        
        # ==========================================
        # –®–ê–ì 8: –î–ê–ù–ù–´–ï –ü–ê–¶–ò–ï–ù–¢–ê
        # ==========================================
        log_step(9, "–ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• –ü–ê–¶–ò–ï–ù–¢–ê –ò–ó –ê–ù–ö–ï–¢–´")
        
        try:
            from save_utils import format_user_profile
            profile_text = await format_user_profile(user_id)
            
            print(f"üë§ –ü—Ä–æ—Ñ–∏–ª—å –ø–∞—Ü–∏–µ–Ω—Ç–∞ –∑–∞–≥—Ä—É–∂–µ–Ω:")
            print(f"   üìè –î–ª–∏–Ω–∞: {len(profile_text)} —Å–∏–º–≤–æ–ª–æ–≤")
            print(f"   üìã –ü—Ä–µ–≤—å—é: {profile_text[:150]}...")
            
        except Exception as e:
            profile_text = "–ü—Ä–æ—Ñ–∏–ª—å –ø–∞—Ü–∏–µ–Ω—Ç–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø—Ä–æ—Ñ–∏–ª—è: {e}")
        
        # ==========================================
        # –®–ê–ì 9: –°–í–û–î–ö–ê –†–ê–ó–ì–û–í–û–†–ê
        # ==========================================
        log_step(10, "–°–í–û–î–ö–ê –ö–û–ù–¢–ï–ö–°–¢–ê –†–ê–ó–ì–û–í–û–†–ê")
        
        try:
            from db_postgresql import get_conversation_summary
            summary_text, _ = await get_conversation_summary(user_id)
            
            print(f"üí≠ –°–≤–æ–¥–∫–∞ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞:")
            print(f"   üìè –î–ª–∏–Ω–∞: {len(summary_text)} —Å–∏–º–≤–æ–ª–æ–≤")
            print(f"   üìã –ü—Ä–µ–≤—å—é: {summary_text[:150]}...")
            
        except Exception as e:
            summary_text = "–°–≤–æ–¥–∫–∞ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞"
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å–≤–æ–¥–∫–∏: {e}")
        
        # ==========================================
        # –®–ê–ì 10: –ü–û–°–õ–ï–î–ù–ò–ô –î–û–ö–£–ú–ï–ù–¢
        # ==========================================
        log_step(11, "–ò–ù–§–û–†–ú–ê–¶–ò–Ø –ò–ó –ü–û–°–õ–ï–î–ù–ï–ì–û –î–û–ö–£–ú–ï–ù–¢–ê")
        
        if last_summary:
            print(f"üìÑ –ü–æ—Å–ª–µ–¥–Ω–∏–π –¥–æ–∫—É–º–µ–Ω—Ç (ID: {last_doc_id}):")
            print(f"   üìè –î–ª–∏–Ω–∞: {len(last_summary)} —Å–∏–º–≤–æ–ª–æ–≤")
            print(f"   üìã –ü—Ä–µ–≤—å—é: {last_summary[:150]}...")
        else:
            print("‚ùå –ü–æ—Å–ª–µ–¥–Ω–∏–π –¥–æ–∫—É–º–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω")
            last_summary = "–ù–µ—Ç –Ω–µ–¥–∞–≤–Ω–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"
        
        # ==========================================
        # –®–ê–ì 11: –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–ë–û–†–ö–ê –ü–†–û–ú–¢–ê
        # ==========================================
        log_step(12, "–§–ò–ù–ê–õ–¨–ù–ê–Ø –°–ë–û–†–ö–ê –ü–†–û–ú–¢–ê")
        
        # –°–æ–±–∏—Ä–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ–º—Ç
        user_prompt_parts = [
            "You have access to the user's health profile, medical documents, imaging reports, conversation history, and memory notes.",
            "Answer only questions related to the user's health. Do not repeat that you're an AI. Do not ask follow-up questions unless critical.",
            "",
            f"üìå Patient profile:\n{profile_text}",
            "",
            f"üß† Conversation summary:\n{summary_text}",
            "",
            f"üìÑ Recent document interpretations:\n{last_summary}",
            "",
            f"üîé Related historical data:\n{chunks_text or '–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞'}",
            "",
            f"Patient: {user_input}"
        ]
        
        final_user_prompt = "\n".join(user_prompt_parts)
        
        print(f"\nüìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–†–û–ú–¢–ê:")
        print(f"   üîß –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º—Ç: {len(system_prompt)} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   üë§ –ü—Ä–æ—Ñ–∏–ª—å: {len(profile_text)} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   üí≠ –°–≤–æ–¥–∫–∞: {len(summary_text)} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   üìÑ –ü–æ—Å–ª–µ–¥–Ω–∏–π –¥–æ–∫—É–º–µ–Ω—Ç: {len(last_summary)} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   üîé –ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ: {len(chunks_text)} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   üìè –û–ë–©–ê–Ø –î–õ–ò–ù–ê: {len(final_user_prompt)} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   üéØ –ü—Ä–∏–º–µ—Ä–Ω–æ —Ç–æ–∫–µ–Ω–æ–≤: {len(final_user_prompt) // 4}")
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ main.py
        return {
            "profile_text": profile_text,
            "summary_text": summary_text, 
            "last_summary": last_summary or "–ù–µ—Ç –Ω–µ–¥–∞–≤–Ω–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤",
            "chunks_text": chunks_text or "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞",
            "chunks_found": len(all_chunks),
            "lang": lang if 'lang' in locals() else 'ru'
        }
        
    except Exception as e:
        log_step(0, "–ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê", f"‚ùå {e}", success=False)
        raise

# üîß –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø –î–õ–Ø –ö–†–ê–¢–ö–û–ì–û –õ–û–ì–ò–†–û–í–ê–ù–ò–Ø
def log_search_summary(vector_count: int, keyword_count: int, final_count: int, 
                      excluded_doc_id: Optional[int] = None):
    """–ö—Ä–∞—Ç–∫–∞—è —Å–≤–æ–¥–∫–∞ –ø–æ–∏—Å–∫–∞ (–¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)"""
    print(f"üß† –ù–∞–π–¥–µ–Ω–æ: {vector_count} –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö + {keyword_count} –∫–ª—é—á–µ–≤—ã—Ö = {final_count} –∏—Ç–æ–≥–æ", end="")
    if excluded_doc_id:
        print(f" (–∏—Å–∫–ª—é—á–µ–Ω –¥–æ–∫.{excluded_doc_id})")
    else:
        print()