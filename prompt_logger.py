import logging
import json
from datetime import datetime
from typing import List, Dict, Tuple, Optional

def log_step(step_num: int, title: str, content: str = "", success: bool = True):
    """–õ–æ–≥–∏—Ä—É–µ—Ç —à–∞–≥ –ø—Ä–æ—Ü–µ—Å—Å–∞ —Å –∫—Ä–∞—Å–∏–≤—ã–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    status = "‚úÖ" if success else "‚ùå"
    separator = "=" * 60
    
    print(f"\n{separator}")
    print(f"{status} –®–ê–ì {step_num}: {title}")
    print(f"{separator}")
    if content:
        print(content)

def log_chunk_info(chunks: list, chunk_type: str):
    """–õ–æ–≥–∏—Ä—É–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —á–∞–Ω–∫–∞—Ö"""
    print(f"\nüìä {chunk_type}: –Ω–∞–π–¥–µ–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤")
    for i, chunk in enumerate(chunks[:3]):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 3
        chunk_text = chunk.get('chunk_text', '')[:100]
        similarity = chunk.get('similarity', chunk.get('rank', 'N/A'))
        if isinstance(similarity, (int, float)):
            print(f"   {i+1}. [üéØ{similarity:.3f}] {chunk_text}...")
        else:
            print(f"   {i+1}. [üìä{similarity}] {chunk_text}...")
    if len(chunks) > 3:
        print(f"   ... –∏ –µ—â–µ {len(chunks) - 3} —á–∞–Ω–∫–æ–≤")

async def get_recent_messages_formatted(user_id: int, limit: int = 6) -> str:
    """
    –ü–æ–ª—É—á–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è –ò–°–ö–õ–Æ–ß–ê–Ø —Ç–µ–∫—É—â–µ–µ (–ø–æ—Å–ª–µ–¥–Ω–µ–µ) —Å–æ–æ–±—â–µ–Ω–∏–µ
    """
    try:
        from db_postgresql import get_last_messages
        
        # –ë–µ—Ä–µ–º –Ω–∞ 1 –±–æ–ª—å—à–µ —á—Ç–æ–±—ã –∏—Å–∫–ª—é—á–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω–µ–µ (—Ç–µ–∫—É—â–µ–µ) —Å–æ–æ–±—â–µ–Ω–∏–µ
        recent_messages = await get_last_messages(user_id, limit=limit + 1)
        
        if not recent_messages:
            return "No recent messages"
        
        # ‚úÖ –ò–°–ö–õ–Æ–ß–ê–ï–ú –ü–û–°–õ–ï–î–ù–ï–ï –°–û–û–ë–©–ï–ù–ò–ï (—Ç–µ–∫—É—â–∏–π –≤–æ–ø—Ä–æ—Å)
        if len(recent_messages) > 1:
            recent_messages = recent_messages[:-1]  # –£–±–∏—Ä–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ
        
        # ‚úÖ –û–ë–ï–°–ü–ï–ß–ò–í–ê–ï–ú –ß–ï–¢–ù–û–ï –ö–û–õ–ò–ß–ï–°–¢–í–û (–ø–∞—Ä—ã USER-BOT)
        if len(recent_messages) % 2 != 0:
            recent_messages = recent_messages[1:]  # –£–±–∏—Ä–∞–µ–º –ø–µ—Ä–≤–æ–µ –µ—Å–ª–∏ –Ω–µ—á–µ—Ç–Ω–æ–µ
        
        formatted_lines = []
        for msg in recent_messages:
            if isinstance(msg, (tuple, list)) and len(msg) >= 2:
                role = "USER" if msg[0] == 'user' else "BOT"
                content = str(msg[1])
                
                # ‚úÖ –£–ú–ù–ê–Ø –û–ß–ò–°–¢–ö–ê HTML –¢–ï–ì–û–í
                import re
                content = re.sub(r'<[^>]+>', '', content)  # –£–±–∏—Ä–∞–µ–º HTML —Ç–µ–≥–∏
                
                # ‚úÖ –û–ë–†–ï–ó–ö–ê –î–û 100 –°–ò–ú–í–û–õ–û–í –ë–ï–ó –†–ê–ó–†–´–í–ê –°–õ–û–í
                if len(content) > 100:
                    content = content[:97]
                    # –ù–∞–π–¥–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –ø—Ä–æ–±–µ–ª —á—Ç–æ–±—ã –Ω–µ —Ä–µ–∑–∞—Ç—å —Å–ª–æ–≤–æ
                    last_space = content.rfind(' ')
                    if last_space > 80:  # –ï—Å–ª–∏ –ø—Ä–æ–±–µ–ª –Ω–µ —Å–ª–∏—à–∫–æ–º –±–ª–∏–∑–∫–æ –∫ –Ω–∞—á–∞–ª—É
                        content = content[:last_space]
                    content += "..."
                
                formatted_lines.append(f"{role}: {content}")
        
        # ‚úÖ –û–ì–†–ê–ù–ò–ß–ò–í–ê–ï–ú –î–û 3 –ü–ê–† (6 —Å–æ–æ–±—â–µ–Ω–∏–π)
        if len(formatted_lines) > 6:
            formatted_lines = formatted_lines[-6:]
        
        return "\n".join(formatted_lines) if formatted_lines else "No recent messages"
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π: {e}")
        return "Recent messages unavailable"

async def get_medical_timeline_simple(user_id: int, limit: int = 6) -> str:
    """
    –ü—Ä–æ—Å—Ç–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –º–µ–¥–∫–∞—Ä—Ç—ã –≤ –∫–æ–º–ø–∞–∫—Ç–Ω–æ–º –≤–∏–¥–µ
    """
    try:
        from db_postgresql import get_db_connection, release_db_connection
        
        conn = await get_db_connection()
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –∑–∞–ø–∏—Å–∏
        rows = await conn.fetch("""
            SELECT event_date, description, importance
            FROM medical_timeline 
            WHERE user_id = $1 
            ORDER BY event_date DESC, created_at DESC
            LIMIT $2
        """, user_id, limit)
        
        if not rows:
            return "Medical timeline: empty"
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∫–æ–º–ø–∞–∫—Ç–Ω–æ
        lines = []
        for row in rows:
            date_str = row['event_date'].strftime('%d.%m.%Y') if row['event_date'] else 'N/A'
            importance = row['importance'] or 'normal'
            description = (row['description'] or '')[:80]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
            
            # –î–æ–±–∞–≤–ª—è–µ–º —ç–º–æ–¥–∑–∏ –≤–∞–∂–Ω–æ—Å—Ç–∏
            emoji = 'üî¥' if importance == 'critical' else 'üü°' if importance == 'important' else '‚ö™'
            lines.append(f"{emoji} {date_str}: {description}")
        
        return "\n".join(lines)
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –º–µ–¥–∫–∞—Ä—Ç—ã: {e}")
        return "Medical timeline: unavailable"
    finally:
        if 'conn' in locals():
            await release_db_connection(conn)

async def get_user_vector_count(user_id: int) -> int:
    """
    üîç –ü–æ–ª—É—á–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–µ–∫—Ç–æ—Ä–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–ü–†–û–°–¢–ê–Ø –≤–µ—Ä—Å–∏—è)
    """
    try:
        from vector_db_postgresql import vector_db
        if not vector_db:
            return 0
            
        conn = await vector_db.db_pool.acquire()
        try:
            result = await conn.fetchval(
                "SELECT COUNT(*) FROM document_vectors WHERE user_id = $1", 
                user_id
            )
            return result or 0
        finally:
            await vector_db.db_pool.release(conn)
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥—Å—á–µ—Ç–∞ –≤–µ–∫—Ç–æ—Ä–æ–≤: {e}")
        return 0

async def get_all_user_chunks(user_id: int, limit: int = 4) -> List[Dict]:
    """
    üì• –ü–æ–ª—É—á–∞–µ—Ç –í–°–ï —á–∞–Ω–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–ü–†–û–°–¢–ê–Ø –≤–µ—Ä—Å–∏—è)
    """
    try:
        from vector_db_postgresql import vector_db
        if not vector_db:
            return []
            
        conn = await vector_db.db_pool.acquire()
        try:
            results = await conn.fetch("""
                SELECT 
                    dv.chunk_text,
                    dv.metadata,
                    dv.keywords,
                    d.title as document_title,
                    d.uploaded_at
                FROM document_vectors dv
                JOIN documents d ON d.id = dv.document_id
                WHERE dv.user_id = $1
                ORDER BY d.uploaded_at DESC
                LIMIT $2
            """, user_id, limit)
            
            chunks = []
            for row in results:
                try:
                    metadata = json.loads(row['metadata']) if row['metadata'] else {}
                except:
                    metadata = {}
                
                chunks.append({
                    "chunk_text": row['chunk_text'],
                    "metadata": metadata,
                    "keywords": row['keywords'],
                    "document_title": row['document_title'],
                    "uploaded_at": row['uploaded_at']
                })
            
            return chunks
        finally:
            await vector_db.db_pool.release(conn)
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –≤—Å–µ—Ö —á–∞–Ω–∫–æ–≤: {e}")
        return []

async def process_user_question_detailed(user_id: int, user_input: str) -> Dict:
    """
    üîç –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø: –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å –ü–†–û–°–¢–û–ô –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π
    
    –õ–æ–≥–∏–∫–∞:
    - 0 –≤–µ–∫—Ç–æ—Ä–æ–≤: –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–æ–∏—Å–∫
    - 1-4 –≤–µ–∫—Ç–æ—Ä–∞: –±–µ—Ä–µ–º –≤—Å–µ –±–µ–∑ –ø–æ–∏—Å–∫–∞  
    - 5+ –≤–µ–∫—Ç–æ—Ä–æ–≤: –¥–µ–ª–∞–µ–º –ø–æ–ª–Ω—ã–π –ø–æ–∏—Å–∫
    
    Returns:
        Dict —Å –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ–º—Ç–∞
    """
    
    # ==========================================
    # –®–ê–ì 1: –ü–û–õ–£–ß–ï–ù–ò–ï –í–û–ü–†–û–°–ê –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø
    # ==========================================
    log_step(1, "–ü–û–õ–£–ß–ï–ù–ò–ï –í–û–ü–†–û–°–ê –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø", 
             f"üë§ –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å {user_id}\nüí¨ –í–æ–ø—Ä–æ—Å: '{user_input}'")
    
    try:
        # ==========================================
        # –®–ê–ì 2: –ü–†–û–í–ï–†–ö–ê –í–ï–ö–¢–û–†–ù–û–ô –ë–ê–ó–´ (–ù–û–í–û–ï!)
        # ==========================================
        log_step(2, "üöÄ –ü–†–û–í–ï–†–ö–ê –í–ï–ö–¢–û–†–ù–û–ô –ë–ê–ó–´")
        
        vector_count = await get_user_vector_count(user_id)
        print(f"üìä –£ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id} –≤–µ–∫—Ç–æ—Ä–æ–≤: {vector_count}")
        
        # ==========================================
        # –®–ê–ì 3: –ü–û–õ–£–ß–ï–ù–ò–ï –ü–†–û–§–ò–õ–Ø –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø
        # ==========================================
        log_step(3, "–ü–û–õ–£–ß–ï–ù–ò–ï –ü–†–û–§–ò–õ–Ø –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø")
        
        try:
            from save_utils import format_user_profile
            profile_text = await format_user_profile(user_id)
            print(f"üë§ –ü—Ä–æ—Ñ–∏–ª—å –ø–æ–ª—É—á–µ–Ω: {len(profile_text)} —Å–∏–º–≤–æ–ª–æ–≤")
        except Exception as e:
            profile_text = "–ü—Ä–æ—Ñ–∏–ª—å –ø–∞—Ü–∏–µ–Ω—Ç–∞ –Ω–µ –∑–∞–ø–æ–ª–Ω–µ–Ω"
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –ø—Ä–æ—Ñ–∏–ª—è: {e}")
        
        # ==========================================
        # –®–ê–ì 4: –ü–û–õ–£–ß–ï–ù–ò–ï –°–í–û–î–ö–ò –†–ê–ó–ì–û–í–û–†–ê
        # ==========================================
        log_step(4, "–ü–û–õ–£–ß–ï–ù–ò–ï –°–í–û–î–ö–ò –†–ê–ó–ì–û–í–û–†–ê")
        
        try:
            from db_postgresql import get_conversation_summary
            summary_text, _ = await get_conversation_summary(user_id)
            
            if not summary_text:
                summary_text = "–ù–æ–≤—ã–π –ø–∞—Ü–∏–µ–Ω—Ç, –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –±–µ—Å–µ–¥ –Ω–µ—Ç"
                
            print(f"üß† –°–≤–æ–¥–∫–∞ –ø–æ–ª—É—á–µ–Ω–∞: {len(summary_text)} —Å–∏–º–≤–æ–ª–æ–≤")
        except Exception as e:
            summary_text = "–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–≤–æ–¥–∫–∏ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞"
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–≤–æ–¥–∫–∏: {e}")
        
        # ==========================================
        # –®–ê–ì 5: –£–ú–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê –í–ï–ö–¢–û–†–û–í
        # ==========================================
        
        if vector_count == 0:
            # üöÄ –ü–£–°–¢–ê–Ø –ë–ê–ó–ê: –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–æ–∏—Å–∫
            log_step(5, "üöÄ –ü–†–û–ü–£–°–ö: –í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –ø—É—Å—Ç–∞—è")
            chunks_text = "–£ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –Ω–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"
            chunks_found = 0
            print("üí∞ –≠–ö–û–ù–û–ú–ò–Ø: –ü—Ä–æ–ø—É—â–µ–Ω—ã GPT –≤—ã–∑–æ–≤—ã –¥–ª—è –ø–æ–∏—Å–∫–∞")
            
        elif vector_count <= 4:
            # üéØ –ú–ê–õ–û –í–ï–ö–¢–û–†–û–í: –±–µ—Ä–µ–º –≤—Å–µ
            log_step(5, f"üéØ –ë–ï–†–ï–ú –í–°–ï: {vector_count} –≤–µ–∫—Ç–æ—Ä–æ–≤")
            
            all_chunks = await get_all_user_chunks(user_id, limit=4)
            
            if all_chunks:
                chunk_texts = [chunk.get("chunk_text", "") for chunk in all_chunks if chunk.get("chunk_text", "").strip()]
                chunks_text = "\n\n".join(chunk_texts)
                chunks_found = len(chunk_texts)
                print(f"üì¶ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {chunks_found} –∑–∞–ø–∏—Å–µ–π –±–µ–∑ –ø–æ–∏—Å–∫–∞")
                print("üí∞ –≠–ö–û–ù–û–ú–ò–Ø: –ü—Ä–æ–ø—É—â–µ–Ω—ã GPT –≤—ã–∑–æ–≤—ã –¥–ª—è –ø–æ–∏—Å–∫–∞")
            else:
                chunks_text = "–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ"
                chunks_found = 0
                
        else:
            # üîç –ú–ù–û–ì–û –í–ï–ö–¢–û–†–û–í: –ø–æ–ª–Ω—ã–π –ø–æ–∏—Å–∫ –∫–∞–∫ –≤ —Ä–∞–±–æ—á–µ–π –≤–µ—Ä—Å–∏–∏
            log_step(5, f"üîç –ü–û–õ–ù–´–ô –ü–û–ò–°–ö: {vector_count} –≤–µ–∫—Ç–æ—Ä–æ–≤")
            
            # –®–ê–ì 5A: –£–õ–£–ß–®–ï–ù–ò–ï –ó–ê–ü–†–û–°–ê
            try:
                from gpt import enrich_query_for_vector_search, extract_keywords
                
                refined_query = await enrich_query_for_vector_search(user_input)
                print(f"üîç –ò—Å—Ö–æ–¥–Ω—ã–π: '{user_input}'")
                print(f"üéØ –£–ª—É—á—à–µ–Ω–Ω—ã–π: '{refined_query}'")
                
                keywords = await extract_keywords(user_input)
                print(f"üîë –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {keywords}")
                
            except Exception as e:
                refined_query = user_input
                keywords = []
                print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞: {e}")
            
            # –®–ê–ì 5B: –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ò–ô –ü–û–ò–°–ö
            try:
                from vector_db_postgresql import search_similar_chunks
                vector_chunks = await search_similar_chunks(user_id, refined_query, limit=10)
                
                if vector_chunks:
                    log_chunk_info(vector_chunks, "–°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ò–ï –ß–ê–ù–ö–ò")
                else:
                    print("‚ùå –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —á–∞–Ω–∫–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
                    
            except Exception as e:
                vector_chunks = []
                print(f"‚ùå –û—à–∏–±–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")
            
            # –®–ê–ì 5C: –ü–û–ò–°–ö –ü–û –ö–õ–Æ–ß–ï–í–´–ú –°–õ–û–í–ê–ú
            try:
                from vector_db_postgresql import keyword_search_chunks
                keyword_list_str = ", ".join(keywords) if keywords else user_input
                keyword_chunks = await keyword_search_chunks(user_id, keyword_list_str, limit=5)
                
                if keyword_chunks:
                    log_chunk_info(keyword_chunks, "–ö–õ–Æ–ß–ï–í–´–ï –ß–ê–ù–ö–ò")
                else:
                    print("‚ùå –ß–∞–Ω–∫–æ–≤ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
                    
            except Exception as e:
                keyword_chunks = []
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º: {e}")
            
            # –®–ê–ì 5D: –ì–ò–ë–†–ò–î–ù–û–ï –†–ê–ù–ñ–ò–†–û–í–ê–ù–ò–ï (–ò–°–ü–†–ê–í–õ–ï–ù–û!)
            try:
                # ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–´–ô –ò–ú–ü–û–†–¢
                from vector_db_postgresql import create_hybrid_ranking
                
                ranked_chunk_texts = create_hybrid_ranking(
                    vector_chunks=vector_chunks,
                    keyword_chunks=keyword_chunks,
                    boost_factor=1.8
                )
                
                selected_chunks = ranked_chunk_texts[:5]
                chunks_text = "\n\n".join(selected_chunks)
                chunks_found = len(selected_chunks)
                
                print(f"\nüì¶ –ì–ò–ë–†–ò–î–ù–´–ô –†–ï–ó–£–õ–¨–¢–ê–¢:")
                print(f"   üî• –†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤: {len(ranked_chunk_texts)}")
                print(f"   üéØ –û—Ç–æ–±—Ä–∞–Ω–æ –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞: {chunks_found}")
                
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
                # Fallback –Ω–∞ –ø—Ä–æ—Å—Ç–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ
                vector_texts = [chunk.get("chunk_text", "") for chunk in vector_chunks[:3] if chunk.get("chunk_text", "").strip()]
                keyword_texts = [chunk.get("chunk_text", "") for chunk in keyword_chunks[:2] if chunk.get("chunk_text", "").strip()]
                all_chunks = list(dict.fromkeys(vector_texts + keyword_texts))
                chunks_text = "\n\n".join(all_chunks[:5])
                chunks_found = len(all_chunks)
                print(f"üì¶ FALLBACK: {chunks_found} —á–∞–Ω–∫–æ–≤")
        
        # ==========================================
        # –®–ê–ì 6: –°–ò–°–¢–ï–ú–ù–´–ô –ü–†–û–ú–¢
        # ==========================================
        log_step(6, "–°–û–ó–î–ê–ù–ò–ï –°–ò–°–¢–ï–ú–ù–û–ì–û –ü–†–û–ú–¢–ê")
        
        try:
            from db_postgresql import get_user_language
            lang = await get_user_language(user_id)
            
            system_prompt = (
                "You are a compassionate and knowledgeable virtual physician who guides the user through their medical journey. "
                "You speak in a friendly, human tone and provide explanations when needed. "
                f"Always respond in the '{lang}' language."
            )
            
            print(f"üåê –Ø–∑—ã–∫ –æ—Ç–≤–µ—Ç–∞: {lang}")
            
        except Exception as e:
            system_prompt = "You are a helpful medical assistant."
            lang = 'ru'
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º—Ç–∞: {e}")
        
        # ==========================================
        # –®–ê–ì 7: –ü–û–õ–£–ß–ï–ù–ò–ï –ú–ï–î–ö–ê–†–¢–´
        # ==========================================
        log_step(7, "–ü–û–õ–£–ß–ï–ù–ò–ï –ú–ï–î–ö–ê–†–¢–´")
        
        try:
            medical_timeline = await get_medical_timeline_simple(user_id, limit=6)
            print(f"üè• –ú–µ–¥–∫–∞—Ä—Ç–∞ –ø–æ–ª—É—á–µ–Ω–∞: {len(medical_timeline)} —Å–∏–º–≤–æ–ª–æ–≤")
        except Exception as e:
            medical_timeline = "Medical timeline: unavailable"
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –º–µ–¥–∫–∞—Ä—Ç—ã: {e}")
        
        # ==========================================
        # –®–ê–ì 8: –ü–û–õ–£–ß–ï–ù–ò–ï –ü–û–°–õ–ï–î–ù–ò–• –°–û–û–ë–©–ï–ù–ò–ô
        # ==========================================
        log_step(8, "–ü–û–õ–£–ß–ï–ù–ò–ï –ü–û–°–õ–ï–î–ù–ò–• –°–û–û–ë–©–ï–ù–ò–ô")
        
        try:
            recent_messages_text = await get_recent_messages_formatted(user_id, limit=6)
            print(f"üí¨ –ü–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ–ª—É—á–µ–Ω—ã: {len(recent_messages_text)} —Å–∏–º–≤–æ–ª–æ–≤")
        except Exception as e:
            recent_messages_text = "Recent messages unavailable"
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π: {e}")


        # ==========================================
        # –®–ê–ì 9: –°–û–ó–î–ê–ù–ò–ï –§–ò–ù–ê–õ–¨–ù–û–ì–û –ü–†–û–ú–¢–ê
        # ==========================================
        log_step(9, "–°–û–ó–î–ê–ù–ò–ï –§–ò–ù–ê–õ–¨–ù–û–ì–û –ü–†–û–ú–¢–ê")
        
        user_prompt_parts = [            
            f"üìå Patient profile:\n{profile_text}",
            "",
            f"üß† Conversation summary:\n{summary_text}",
            "",
            f"üè• Medical timeline:\n{medical_timeline}",  # ‚Üê –î–û–ë–ê–í–ò–¢–¨ –ú–ï–î–ö–ê–†–¢–£
            "",
            f"üîé Related historical data:\n{chunks_text or '–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞'}",
            "",
            f"üí¨ Recent messages (last 3 pairs):\n{recent_messages_text}",  # ‚Üê –î–û–ë–ê–í–ò–¢–¨ –ü–û–°–õ–ï–î–ù–ò–ï –°–û–û–ë–©–ï–ù–ò–Ø
            "",
            f"Patient: {user_input}"
        ]
        
        final_user_prompt = "\n".join(user_prompt_parts)
        
        print(f"\nüìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–†–û–ú–ü–¢–ê:")
        print(f"   üîß –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º—Ç: {len(system_prompt)} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   üë§ –ü—Ä–æ—Ñ–∏–ª—å: {len(profile_text)} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   üí≠ –°–≤–æ–¥–∫–∞: {len(summary_text)} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   üè• –ú–µ–¥–∫–∞—Ä—Ç–∞: {len(medical_timeline)} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   üí¨ –ü–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è: {len(recent_messages_text)} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   üîé –ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ: {len(chunks_text)} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   üìè –û–ë–©–ê–Ø –î–õ–ò–ù–ê: {len(final_user_prompt)} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   üéØ –ü—Ä–∏–º–µ—Ä–Ω–æ —Ç–æ–∫–µ–Ω–æ–≤: {len(final_user_prompt) // 4}")
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        if vector_count <= 4:
            print(f"\nüí∞ –≠–ö–û–ù–û–ú–ò–Ø:")
            print(f"   üìä –í–µ–∫—Ç–æ—Ä–æ–≤: {vector_count}")
            print(f"   üí∏ –ü—Ä–æ–ø—É—â–µ–Ω–æ GPT –≤—ã–∑–æ–≤–æ–≤: 3")
            print(f"   ‚ö° –†–µ–∂–∏–º: —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π")
        
        return {
            "profile_text": profile_text,
            "summary_text": summary_text,
            "medical_timeline": medical_timeline,
            "recent_messages": recent_messages_text,
            "chunks_text": chunks_text or "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞",
            "chunks_found": chunks_found,
            "lang": lang if 'lang' in locals() else 'ru',
            "context_text": final_user_prompt,
            "vector_count": vector_count
        }
        
    except Exception as e:
        log_step(0, "–ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê", f"‚ùå {e}", success=False)
        raise