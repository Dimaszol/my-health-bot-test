# prompt_logger.py - –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –í–ï–†–°–ò–Ø —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã

import logging
import json
from datetime import datetime
from typing import List, Dict, Tuple, Optional

def log_step(step_num: int, title: str, content: str = "", success: bool = True):
    """–õ–æ–≥–∏—Ä—É–µ—Ç —à–∞–≥ –ø—Ä–æ—Ü–µ—Å—Å–∞ —Å –∫—Ä–∞—Å–∏–≤—ã–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    status = "‚úÖ" if success else "‚ùå"
    separator = "=" * 60
    
    print(f"\n{separator}")
    print(f"{status} –®–ê–ì {step_num}: {title}")
    print(f"{separator}")
    if content:
        print(content)

def log_chunk_info(chunks: list, chunk_type: str):
    """–õ–æ–≥–∏—Ä—É–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —á–∞–Ω–∫–∞—Ö"""
    print(f"\nüìä {chunk_type}: –Ω–∞–π–¥–µ–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤")
    for i, chunk in enumerate(chunks[:3]):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 3
        chunk_text = chunk.get('chunk_text', '')[:100]
        similarity = chunk.get('similarity', chunk.get('rank', 'N/A'))
        if isinstance(similarity, (int, float)):
            print(f"   {i+1}. [üéØ{similarity:.3f}] {chunk_text}...")
        else:
            print(f"   {i+1}. [üìä{similarity}] {chunk_text}...")
    if len(chunks) > 3:
        print(f"   ... –∏ –µ—â–µ {len(chunks) - 3} —á–∞–Ω–∫–æ–≤")

async def get_user_vector_count(user_id: int) -> int:
    """
    üîç –ù–û–í–ê–Ø –§–£–ù–ö–¶–ò–Ø: –ü–æ–ª—É—á–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–µ–∫—Ç–æ—Ä–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    
    Returns:
        int: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–µ–∫—Ç–æ—Ä–æ–≤ –≤ –±–∞–∑–µ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    """
    try:
        from vector_db_postgresql import vector_db
        if not vector_db:
            print("‚ùå –í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
            return 0
            
        conn = await vector_db.db_pool.acquire()
        try:
            result = await conn.fetchval("""
                SELECT COUNT(*) 
                FROM document_vectors 
                WHERE user_id = $1
            """, user_id)
            
            count = result or 0
            print(f"üìä –£ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id} –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–µ: {count} –∑–∞–ø–∏—Å–µ–π")
            return count
            
        finally:
            await vector_db.db_pool.release(conn)
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥—Å—á–µ—Ç–∞ –≤–µ–∫—Ç–æ—Ä–æ–≤ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}: {e}")
        return 0

async def get_all_user_vectors(user_id: int, limit: int = 4) -> List[Dict]:
    """
    üì• –ù–û–í–ê–Ø –§–£–ù–ö–¶–ò–Ø: –ü–æ–ª—É—á–∞–µ—Ç –í–°–ï –≤–µ–∫—Ç–æ—Ä—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–¥–ª—è –º–∞–ª—ã—Ö –±–∞–∑)
    
    Args:
        user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π
        
    Returns:
        List[Dict]: –í—Å–µ –∑–∞–ø–∏—Å–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏–∑ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã
    """
    try:
        from vector_db_postgresql import vector_db
        if not vector_db:
            return []
            
        conn = await vector_db.db_pool.acquire()
        try:
            results = await conn.fetch("""
                SELECT 
                    dv.chunk_text,
                    dv.metadata,
                    dv.keywords,
                    d.title as document_title,
                    d.uploaded_at
                FROM document_vectors dv
                JOIN documents d ON d.id = dv.document_id
                WHERE dv.user_id = $1
                ORDER BY d.uploaded_at DESC, dv.id DESC
                LIMIT $2
            """, user_id, limit)
            
            chunks = []
            for row in results:
                try:
                    metadata = json.loads(row['metadata']) if row['metadata'] else {}
                except (json.JSONDecodeError, TypeError):
                    metadata = {}
                
                chunk_data = {
                    "chunk_text": row['chunk_text'],
                    "metadata": metadata,
                    "keywords": row['keywords'],
                    "document_title": row['document_title'],
                    "uploaded_at": row['uploaded_at'],
                    "similarity": 1.0,  # –í—Å–µ –∑–∞–ø–∏—Å–∏ –æ–¥–∏–Ω–∞–∫–æ–≤–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã
                    "final_score": 1.0
                }
                chunks.append(chunk_data)
            
            print(f"üì¶ –ü–æ–ª—É—á–µ–Ω–æ {len(chunks)} –∑–∞–ø–∏—Å–µ–π –∏–∑ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}")
            return chunks
            
        finally:
            await vector_db.db_pool.release(conn)
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –≤—Å–µ—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}: {e}")
        return []

async def process_user_question_detailed(user_id: int, user_input: str) -> Dict:
    """
    üîç –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø: –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å —É–º–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π
    
    –õ–æ–≥–∏–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:
    - 0 –≤–µ–∫—Ç–æ—Ä–æ–≤: –ù–ï –≤—ã–∑—ã–≤–∞–µ–º GPT –¥–ª—è –ø–æ–∏—Å–∫–∞, —Å—Ä–∞–∑—É –æ—Ç–≤–µ—á–∞–µ–º
    - 1-4 –≤–µ–∫—Ç–æ—Ä–∞: –ù–ï –¥–µ–ª–∞–µ–º –ø–æ–∏—Å–∫, –±–µ—Ä–µ–º –í–°–ï –≤–µ–∫—Ç–æ—Ä—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è  
    - 5+ –≤–µ–∫—Ç–æ—Ä–æ–≤: –î–µ–ª–∞–µ–º –ø–æ–ª–Ω—ã–π –ø–æ–∏—Å–∫ –∫–∞–∫ –æ–±—ã—á–Ω–æ
    
    Returns:
        Dict —Å –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞
    """
    
    # ==========================================
    # –®–ê–ì 1: –ü–û–õ–£–ß–ï–ù–ò–ï –í–û–ü–†–û–°–ê –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø
    # ==========================================
    log_step(1, "–ü–û–õ–£–ß–ï–ù–ò–ï –í–û–ü–†–û–°–ê –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø", 
             f"üë§ –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å {user_id}\nüí¨ –í–æ–ø—Ä–æ—Å: '{user_input}'")
    
    try:
        # ==========================================
        # –®–ê–ì 2: –ü–†–û–í–ï–†–ö–ê –í–ï–ö–¢–û–†–ù–û–ô –ë–ê–ó–´ (–ù–û–í–û–ï!)
        # ==========================================
        log_step(2, "üöÄ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ü–†–û–í–ï–†–ö–ê –í–ï–ö–¢–û–†–ù–û–ô –ë–ê–ó–´")
        
        vector_count = await get_user_vector_count(user_id)
        
        if vector_count == 0:
            print("üéØ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –ø—É—Å—Ç–∞—è - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –í–°–ï GPT –≤—ã–∑–æ–≤—ã –¥–ª—è –ø–æ–∏—Å–∫–∞")
            search_mode = "empty"
        elif vector_count <= 4:
            print(f"üéØ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ú–∞–ª–æ –≤–µ–∫—Ç–æ—Ä–æ–≤ ({vector_count}) - –±–µ—Ä–µ–º –í–°–ï –±–µ–∑ –ø–æ–∏—Å–∫–∞")
            search_mode = "take_all"
        else:
            print(f"üéØ –°–¢–ê–ù–î–ê–†–¢–ù–´–ô –†–ï–ñ–ò–ú: –ú–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–æ–≤ ({vector_count}) - –¥–µ–ª–∞–µ–º –ø–æ–ª–Ω—ã–π –ø–æ–∏—Å–∫")
            search_mode = "full_search"
        
        # ==========================================
        # –®–ê–ì 3: –ü–û–õ–£–ß–ï–ù–ò–ï –ü–†–û–§–ò–õ–Ø –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø
        # ==========================================
        log_step(3, "–ü–û–õ–£–ß–ï–ù–ò–ï –ü–†–û–§–ò–õ–Ø –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø")
        
        try:
            from save_utils import format_user_profile
            profile_text = await format_user_profile(user_id)
            print(f"üë§ –ü—Ä–æ—Ñ–∏–ª—å –ø–æ–ª—É—á–µ–Ω: {len(profile_text)} —Å–∏–º–≤–æ–ª–æ–≤")
        except Exception as e:
            profile_text = "–ü—Ä–æ—Ñ–∏–ª—å –ø–∞—Ü–∏–µ–Ω—Ç–∞ –Ω–µ –∑–∞–ø–æ–ª–Ω–µ–Ω"
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –ø—Ä–æ—Ñ–∏–ª—è: {e}")
        
        # ==========================================
        # –®–ê–ì 4: –ü–û–õ–£–ß–ï–ù–ò–ï –°–í–û–î–ö–ò –†–ê–ó–ì–û–í–û–†–ê
        # ==========================================
        log_step(4, "–ü–û–õ–£–ß–ï–ù–ò–ï –°–í–û–î–ö–ò –†–ê–ó–ì–û–í–û–†–ê")
        
        try:
            from db_postgresql import get_conversation_summary
            summary_text, _ = await get_conversation_summary(user_id)
            
            if not summary_text:
                summary_text = "–ù–æ–≤—ã–π –ø–∞—Ü–∏–µ–Ω—Ç, –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –±–µ—Å–µ–¥ –Ω–µ—Ç"
                
            print(f"üß† –°–≤–æ–¥–∫–∞ –ø–æ–ª—É—á–µ–Ω–∞: {len(summary_text)} —Å–∏–º–≤–æ–ª–æ–≤")
        except Exception as e:
            summary_text = "–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–≤–æ–¥–∫–∏ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞"
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–≤–æ–¥–∫–∏: {e}")
        
        # ==========================================
        # –®–ê–ì 5: –£–ú–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê –í–ï–ö–¢–û–†–ù–û–ô –ë–ê–ó–´
        # ==========================================
        
        if search_mode == "empty":
            # üöÄ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ü—É—Å—Ç–∞—è –±–∞–∑–∞ - –Ω–∏–∫–∞–∫–∏—Ö –≤—ã–∑–æ–≤–æ–≤ GPT
            log_step(5, "üöÄ –ü–†–û–ü–£–°–ö: –í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –ø—É—Å—Ç–∞—è")
            chunks_text = "–£ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –Ω–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"
            chunks_found = 0
            print("üí∞ –≠–ö–û–ù–û–ú–ò–Ø: –ü—Ä–æ–ø—É—â–µ–Ω–æ 3 –≤—ã–∑–æ–≤–∞ GPT (—Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ + –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ + —ç–º–±–µ–¥–¥–∏–Ω–≥)")
            
        elif search_mode == "take_all":
            # üéØ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ú–∞–ª–æ –≤–µ–∫—Ç–æ—Ä–æ–≤ - –±–µ—Ä–µ–º –≤—Å–µ –±–µ–∑ –ø–æ–∏—Å–∫–∞
            log_step(5, f"üéØ –£–ú–ù–ê–Ø –ó–ê–ì–†–£–ó–ö–ê: –ë–µ—Ä–µ–º –≤—Å–µ {vector_count} –∑–∞–ø–∏—Å–µ–π –±–µ–∑ –ø–æ–∏—Å–∫–∞")
            
            all_chunks = await get_all_user_vectors(user_id, limit=4)
            
            if all_chunks:
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ç–µ–∫—Å—Ç
                chunk_texts = []
                for chunk in all_chunks:
                    chunk_text = chunk.get("chunk_text", "")
                    if chunk_text.strip():
                        chunk_texts.append(chunk_text)
                
                chunks_text = "\n\n".join(chunk_texts)
                chunks_found = len(chunk_texts)
                
                print(f"üì¶ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {chunks_found} –∑–∞–ø–∏—Å–µ–π ({len(chunks_text)} —Å–∏–º–≤–æ–ª–æ–≤)")
                print("üí∞ –≠–ö–û–ù–û–ú–ò–Ø: –ü—Ä–æ–ø—É—â–µ–Ω–æ 3 –≤—ã–∑–æ–≤–∞ GPT (—Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ + –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ + —ç–º–±–µ–¥–¥–∏–Ω–≥)")
            else:
                chunks_text = "–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã"
                chunks_found = 0
                
        else:
            # üîç –ü–û–õ–ù–´–ô –ü–û–ò–°–ö: –ú–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–æ–≤ - –¥–µ–ª–∞–µ–º –∫–∞–∫ –æ–±—ã—á–Ω–æ
            log_step(5, "üîç –ü–û–õ–ù–´–ô –ü–û–ò–°–ö: –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ –∏ –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫")
            
            # –®–ê–ì 5A: –£–õ–£–ß–®–ï–ù–ò–ï –ó–ê–ü–†–û–°–ê –î–õ–Ø –ü–û–ò–°–ö–ê
            try:
                from gpt import enrich_query_for_vector_search, extract_keywords
                
                refined_query = await enrich_query_for_vector_search(user_input)
                print(f"üîç –ò—Å—Ö–æ–¥–Ω—ã–π: '{user_input}'")
                print(f"üéØ –£–ª—É—á—à–µ–Ω–Ω—ã–π: '{refined_query}'")
                
                keywords = await extract_keywords(user_input)
                print(f"üîë –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {keywords}")
                
            except Exception as e:
                refined_query = user_input
                keywords = []
                print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π: {e}")
            
            # –®–ê–ì 5B: –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ò–ô –ü–û–ò–°–ö
            try:
                from vector_db_postgresql import search_similar_chunks
                vector_chunks = await search_similar_chunks(user_id, refined_query, limit=10)
                
                if vector_chunks:
                    log_chunk_info(vector_chunks, "–°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ò–ï –ß–ê–ù–ö–ò")
                else:
                    print("‚ùå –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —á–∞–Ω–∫–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
                    
            except Exception as e:
                vector_chunks = []
                print(f"‚ùå –û—à–∏–±–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")
            
            # –®–ê–ì 5C: –ü–û–ò–°–ö –ü–û –ö–õ–Æ–ß–ï–í–´–ú –°–õ–û–í–ê–ú
            try:
                from vector_db_postgresql import keyword_search_chunks
                keyword_list_str = ", ".join(keywords) if keywords else user_input
                keyword_chunks = await keyword_search_chunks(user_id, keyword_list_str, limit=5)
                
                if keyword_chunks:
                    log_chunk_info(keyword_chunks, "–ö–õ–Æ–ß–ï–í–´–ï –ß–ê–ù–ö–ò")
                else:
                    print("‚ùå –ß–∞–Ω–∫–æ–≤ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
                    
            except Exception as e:
                keyword_chunks = []
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º: {e}")
            
            # –®–ê–ì 5D: –ì–ò–ë–†–ò–î–ù–û–ï –†–ê–ù–ñ–ò–†–û–í–ê–ù–ò–ï
            try:
                from save_utils import rank_chunks_hybrid
                
                ranked_chunk_texts = rank_chunks_hybrid(
                    vector_chunks=vector_chunks,
                    keyword_chunks=keyword_chunks,
                    query=user_input,
                    max_chunks=6
                )
                
                selected_chunks = ranked_chunk_texts[:6]
                chunks_text = "\n\n".join(selected_chunks)
                chunks_found = len(selected_chunks)
                
                print(f"\nüì¶ –ò–¢–û–ì–û–í–´–ô –†–ï–ó–£–õ–¨–¢–ê–¢ –ì–ò–ë–†–ò–î–ù–û–ì–û –ü–û–ò–°–ö–ê:")
                print(f"   üî• –†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤: {len(ranked_chunk_texts)}")
                print(f"   üéØ –û—Ç–æ–±—Ä–∞–Ω–æ –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞: {chunks_found}")
                print(f"   üìÑ –°–∏–º–≤–æ–ª–æ–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {len(chunks_text)}")
                
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
                # Fallback –Ω–∞ –ø—Ä–æ—Å—Ç–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ
                
                def filter_chunks_simple(chunks, limit=5):
                    filtered_texts = []
                    for chunk in chunks:
                        chunk_text = chunk.get("chunk_text", "")
                        if chunk_text.strip():
                            filtered_texts.append(chunk_text)
                            if len(filtered_texts) >= limit:
                                break
                    return filtered_texts

                vector_texts = filter_chunks_simple(vector_chunks, limit=3)
                keyword_texts = filter_chunks_simple(keyword_chunks, limit=2)
                all_chunks = list(dict.fromkeys(vector_texts + keyword_texts))
                chunks_text = "\n\n".join(all_chunks[:5])
                chunks_found = len(all_chunks)
                
                print(f"üì¶ FALLBACK: {chunks_found} —á–∞–Ω–∫–æ–≤ (–ø—Ä–æ—Å—Ç–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ)")
        
        # ==========================================
        # –®–ê–ì 6: –°–ò–°–¢–ï–ú–ù–´–ô –ü–†–û–ú–¢
        # ==========================================
        log_step(6, "–°–û–ó–î–ê–ù–ò–ï –°–ò–°–¢–ï–ú–ù–û–ì–û –ü–†–û–ú–ü–¢–ê")
        
        try:
            from db_postgresql import get_user_language
            lang = await get_user_language(user_id)
            
            system_prompt = (
                "You are a compassionate and knowledgeable virtual physician who guides the user through their medical journey. "
                "You speak in a friendly, human tone and provide explanations when needed. "
                f"Always respond in the '{lang}' language."
            )
            
            print(f"üåê –Ø–∑—ã–∫ –æ—Ç–≤–µ—Ç–∞: {lang}")
            print(f"üìè –î–ª–∏–Ω–∞ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º—Ç–∞: {len(system_prompt)} —Å–∏–º–≤–æ–ª–æ–≤")
            
        except Exception as e:
            system_prompt = "You are a helpful medical assistant."
            lang = 'ru'
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º—Ç–∞: {e}")
        
        # ==========================================
        # –®–ê–ì 7: –°–û–ó–î–ê–ù–ò–ï –§–ò–ù–ê–õ–¨–ù–û–ì–û –ü–†–û–ú–ü–¢–ê
        # ==========================================
        log_step(7, "–°–û–ó–î–ê–ù–ò–ï –§–ò–ù–ê–õ–¨–ù–û–ì–û –ü–†–û–ú–ü–¢–ê")
        
        user_prompt_parts = [
            "Answer only questions related to the user's health. Do not repeat that you're an AI. Do not ask follow-up questions unless critical.",
            "",
            f"üìå Patient profile:\n{profile_text}",
            "",
            f"üß† Conversation summary:\n{summary_text}",
            "",
            f"üîé Related historical data:\n{chunks_text or '–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞'}",
            "",
            f"Patient: {user_input}"
        ]
        
        final_user_prompt = "\n".join(user_prompt_parts)
        
        print(f"\nüìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–†–û–ú–ü–¢–ê:")
        print(f"   üîß –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º—Ç: {len(system_prompt)} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   üë§ –ü—Ä–æ—Ñ–∏–ª—å: {len(profile_text)} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   üí≠ –°–≤–æ–¥–∫–∞: {len(summary_text)} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   üîé –ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ: {len(chunks_text)} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   üìè –û–ë–©–ê–Ø –î–õ–ò–ù–ê: {len(final_user_prompt)} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   üéØ –ü—Ä–∏–º–µ—Ä–Ω–æ —Ç–æ–∫–µ–Ω–æ–≤: {len(final_user_prompt) // 4}")
        
        # ==========================================
        # –®–ê–ì 8: –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–í–û–î–ö–ê –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ô
        # ==========================================
        
        if search_mode in ["empty", "take_all"]:
            print(f"\nüí∞ –ò–¢–û–ì–û–í–ê–Ø –≠–ö–û–ù–û–ú–ò–Ø:")
            print(f"   üöÄ –†–µ–∂–∏–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: {search_mode}")
            print(f"   üí∏ –ü—Ä–æ–ø—É—â–µ–Ω–æ –≤—ã–∑–æ–≤–æ–≤ GPT: 3")
            print(f"   üìä –í–µ–∫—Ç–æ—Ä–æ–≤ –≤ –±–∞–∑–µ: {vector_count}")
            print(f"   ‚ö° –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∫—Ä–∞—â–µ–Ω–æ")
        else:
            print(f"\nüîç –ü–û–õ–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê:")
            print(f"   üìä –í–µ–∫—Ç–æ—Ä–æ–≤ –≤ –±–∞–∑–µ: {vector_count}")
            print(f"   üß† –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã –≤—Å–µ GPT –≤—ã–∑–æ–≤—ã")
            print(f"   ‚ö° –†–µ–∂–∏–º: –ø–æ–ª–Ω—ã–π –ø–æ–∏—Å–∫")
        
        return {
            "profile_text": profile_text,
            "summary_text": summary_text, 
            "chunks_text": chunks_text or "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞",
            "chunks_found": chunks_found,
            "lang": lang if 'lang' in locals() else 'ru',
            "context_text": final_user_prompt,
            "search_mode": search_mode,  # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            "vector_count": vector_count  # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        }
        
    except Exception as e:
        log_step(0, "–ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê", f"‚ùå {e}", success=False)
        raise