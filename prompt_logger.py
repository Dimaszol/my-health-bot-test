# prompt_logger.py - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø –±–µ–∑ last_summary

import logging
import json
from datetime import datetime
from typing import List, Dict, Tuple, Optional

def log_step(step_num: int, title: str, content: str = "", success: bool = True):
    """–õ–æ–≥–∏—Ä—É–µ—Ç —à–∞–≥ –ø—Ä–æ—Ü–µ—Å—Å–∞ —Å –∫—Ä–∞—Å–∏–≤—ã–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    status = "‚úÖ" if success else "‚ùå"
    separator = "=" * 60
    
    print(f"\n{separator}")
    print(f"{status} –®–ê–ì {step_num}: {title}")
    print(f"{separator}")
    if content:
        print(content)

def log_chunk_info(chunks: list, chunk_type: str):
    """–õ–æ–≥–∏—Ä—É–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —á–∞–Ω–∫–∞—Ö"""
    print(f"\nüìä {chunk_type}: –Ω–∞–π–¥–µ–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤")
    for i, chunk in enumerate(chunks[:3]):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 3
        chunk_text = chunk.get('chunk_text', '')[:100]
        similarity = chunk.get('similarity', chunk.get('rank', 'N/A'))
        if isinstance(similarity, (int, float)):
            print(f"   {i+1}. [üéØ{similarity:.3f}] {chunk_text}...")
        else:
            print(f"   {i+1}. [üìä{similarity}] {chunk_text}...")
    if len(chunks) > 3:
        print(f"   ... –∏ –µ—â–µ {len(chunks) - 3} —á–∞–Ω–∫–æ–≤")

def filter_chunks_with_logging(chunks: list, chunk_type: str, limit=5) -> list:
    """‚úÖ –£–ü–†–û–©–ï–ù–ù–ê–Ø —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —á–∞–Ω–∫–æ–≤ –±–µ–∑ –∏—Å–∫–ª—é—á–µ–Ω–∏–π –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    print(f"\nüîç –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è {chunk_type}:")
    print(f"   üì• –í—Ö–æ–¥—è—â–∏—Ö —á–∞–Ω–∫–æ–≤: {len(chunks)}")
    
    filtered_texts = []
    
    for chunk in chunks:
        chunk_text = chunk.get("chunk_text", "")
        
        # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–µ–ø—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç
        if chunk_text.strip():
            filtered_texts.append(chunk_text)
            if len(filtered_texts) >= limit:
                break
    
    print(f"   ‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã—Ö —á–∞–Ω–∫–æ–≤: {len(filtered_texts)}")
    
    return filtered_texts

async def process_user_question_detailed(user_id: int, user_input: str) -> Dict:
    """
    üîç –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø: –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å –ø–æ–¥—Ä–æ–±–Ω—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º
    ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø –±–µ–∑ last_summary
    
    Returns:
        Dict —Å –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ–º—Ç–∞
    """
    
    # ==========================================
    # –®–ê–ì 1: –ü–û–õ–£–ß–ï–ù–ò–ï –í–û–ü–†–û–°–ê –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø
    # ==========================================
    log_step(1, "–ü–û–õ–£–ß–ï–ù–ò–ï –í–û–ü–†–û–°–ê –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø", 
             f"üë§ –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å {user_id}\nüí¨ –í–æ–ø—Ä–æ—Å: '{user_input}'")
    
    try:
        # ==========================================
        # –®–ê–ì 2: –ü–û–õ–£–ß–ï–ù–ò–ï –ü–†–û–§–ò–õ–Ø –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø
        # ==========================================
        log_step(2, "–ü–û–õ–£–ß–ï–ù–ò–ï –ü–†–û–§–ò–õ–Ø –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø")
        
        try:
            from save_utils import format_user_profile
            profile_text = await format_user_profile(user_id)
            print(f"üë§ –ü—Ä–æ—Ñ–∏–ª—å –ø–æ–ª—É—á–µ–Ω: {len(profile_text)} —Å–∏–º–≤–æ–ª–æ–≤")
        except Exception as e:
            profile_text = "–ü—Ä–æ—Ñ–∏–ª—å –ø–∞—Ü–∏–µ–Ω—Ç–∞ –Ω–µ –∑–∞–ø–æ–ª–Ω–µ–Ω"
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –ø—Ä–æ—Ñ–∏–ª—è: {e}")
        
        # ==========================================
        # –®–ê–ì 3: –ü–û–õ–£–ß–ï–ù–ò–ï –°–í–û–î–ö–ò –†–ê–ó–ì–û–í–û–†–ê
        # ==========================================
        log_step(3, "–ü–û–õ–£–ß–ï–ù–ò–ï –°–í–û–î–ö–ò –†–ê–ó–ì–û–í–û–†–ê")
        
        try:
            from db_postgresql import get_conversation_summary
            summary_text, _ = await get_conversation_summary(user_id)
            
            # –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
            if not summary_text:
                summary_text = "–ù–æ–≤—ã–π –ø–∞—Ü–∏–µ–Ω—Ç, –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –±–µ—Å–µ–¥ –Ω–µ—Ç"
                
            print(f"üß† –°–≤–æ–¥–∫–∞ –ø–æ–ª—É—á–µ–Ω–∞: {len(summary_text)} —Å–∏–º–≤–æ–ª–æ–≤")
        except Exception as e:
            summary_text = "–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–≤–æ–¥–∫–∏ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞"
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–≤–æ–¥–∫–∏: {e}")
        
        # ==========================================
        # –®–ê–ì 4: –£–õ–£–ß–®–ï–ù–ò–ï –ó–ê–ü–†–û–°–ê –î–õ–Ø –ü–û–ò–°–ö–ê
        # ==========================================
        log_step(4, "–£–õ–£–ß–®–ï–ù–ò–ï –ó–ê–ü–†–û–°–ê –î–õ–Ø –í–ï–ö–¢–û–†–ù–û–ì–û –ü–û–ò–°–ö–ê")
        
        try:
            from gpt import enrich_query_for_vector_search, extract_keywords
            
            # –£–ª—É—á—à–∞–µ–º –∑–∞–ø—Ä–æ—Å –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
            refined_query = await enrich_query_for_vector_search(user_input)
            print(f"üîç –ò—Å—Ö–æ–¥–Ω—ã–π: '{user_input}'")
            print(f"üéØ –£–ª—É—á—à–µ–Ω–Ω—ã–π: '{refined_query}'")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            keywords = await extract_keywords(user_input)
            print(f"üîë –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {keywords}")
            
        except Exception as e:
            refined_query = user_input
            keywords = []
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π: {e}")
        
        # ‚ùå –£–ë–ò–†–ê–ï–ú –®–ê–ì –° –ü–û–õ–£–ß–ï–ù–ò–ï–ú last_summary
        # –ë–æ–ª—å—à–µ –ù–ï –ø–æ–ª—É—á–∞–µ–º –∏ –ù–ï –∏—Å–∫–ª—é—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –¥–æ–∫—É–º–µ–Ω—Ç
        
        # ==========================================
        # –®–ê–ì 5A: –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ò–ô –ü–û–ò–°–ö
        # ==========================================
        log_step(5, "–°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ò–ô –ü–û–ò–°–ö –ü–û –í–ï–ö–¢–û–†–ù–û–ô –ë–ê–ó–ï")
        
        try:
            from vector_db_postgresql import search_similar_chunks
            # ‚úÖ –ò–©–ï–ú –ü–û –í–°–ï–ú –î–û–ö–£–ú–ï–ù–¢–ê–ú –±–µ–∑ –∏—Å–∫–ª—é—á–µ–Ω–∏–π
            vector_chunks = await search_similar_chunks(user_id, refined_query, limit=10)
            
            if vector_chunks:
                log_chunk_info(vector_chunks, "–°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ò–ï –ß–ê–ù–ö–ò")
            else:
                print("‚ùå –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —á–∞–Ω–∫–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
                
        except Exception as e:
            vector_chunks = []
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")
        
        # ==========================================
        # –®–ê–ì 5B: –ü–û–ò–°–ö –ü–û –ö–õ–Æ–ß–ï–í–´–ú –°–õ–û–í–ê–ú
        # ==========================================
        log_step(6, "–ü–û–ò–°–ö –ü–û –ö–õ–Æ–ß–ï–í–´–ú –°–õ–û–í–ê–ú")
        
        try:
            from vector_db_postgresql import keyword_search_chunks
                       
            # –ü–µ—Ä–µ–¥–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞, –∞ –Ω–µ –∏—Å—Ö–æ–¥–Ω—ã–π –≤–æ–ø—Ä–æ—Å
            keywords_string = ", ".join(keywords) if keywords else user_input
            # ‚úÖ –ò–©–ï–ú –ü–û –í–°–ï–ú –î–û–ö–£–ú–ï–ù–¢–ê–ú –±–µ–∑ –∏—Å–∫–ª—é—á–µ–Ω–∏–π
            keyword_chunks = await keyword_search_chunks(user_id, keywords_string, limit=10)
            
            print(f"üîç –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º: '{keywords_string}'")
            
            if keyword_chunks:
                log_chunk_info(keyword_chunks, "–ö–õ–Æ–ß–ï–í–´–ï –ß–ê–ù–ö–ò")
            else:
                print("‚ùå –ß–∞–Ω–∫–æ–≤ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
                
        except Exception as e:
            keyword_chunks = []
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º: {e}")
        
        # ==========================================
        # –®–ê–ì 6: –§–ò–õ–¨–¢–†–ê–¶–ò–Ø –ò –û–ë–™–ï–î–ò–ù–ï–ù–ò–ï
        # ==========================================
        log_step(7, "–§–ò–õ–¨–¢–†–ê–¶–ò–Ø –ò –û–ë–™–ï–î–ò–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
        
        # ‚úÖ –§–ò–õ–¨–¢–†–£–ï–ú –ë–ï–ó –ò–°–ö–õ–Æ–ß–ï–ù–ò–ô –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º
        vector_texts = filter_chunks_with_logging(
            vector_chunks, "–°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ò–•", limit=4
        )
        
        keyword_texts = filter_chunks_with_logging(
            keyword_chunks, "–ö–õ–Æ–ß–ï–í–´–•", limit=2
        )
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∏ —É–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        all_chunks = list(dict.fromkeys(vector_texts + keyword_texts))
        chunks_text = "\n\n".join(all_chunks[:6])
        
        print(f"\nüì¶ –ò–¢–û–ì–û–í–´–ô –†–ï–ó–£–õ–¨–¢–ê–¢ –ü–û–ò–°–ö–ê:")
        print(f"   üß† –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —á–∞–Ω–∫–æ–≤: {len(vector_texts)}")
        print(f"   üîë –ö–ª—é—á–µ–≤—ã—Ö —á–∞–Ω–∫–æ–≤: {len(keyword_texts)}")
        print(f"   üìã –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —á–∞–Ω–∫–æ–≤: {len(all_chunks)}")
        print(f"   üìÑ –°–∏–º–≤–æ–ª–æ–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {len(chunks_text)}")
        
        # ==========================================
        # –®–ê–ì 7: –°–ò–°–¢–ï–ú–ù–´–ô –ü–†–û–ú–¢
        # ==========================================
        log_step(8, "–°–û–ó–î–ê–ù–ò–ï –°–ò–°–¢–ï–ú–ù–û–ì–û –ü–†–û–ú–¢–ê")
        
        try:
            from db_postgresql import get_user_language
            lang = await get_user_language(user_id)
            
            system_prompt = (
                "You are a compassionate and knowledgeable virtual physician who guides the user through their medical journey. "
                "You speak in a friendly, human tone and provide explanations when needed. "
                f"Always respond in the '{lang}' language."
            )
            
            print(f"üåê –Ø–∑—ã–∫ –æ—Ç–≤–µ—Ç–∞: {lang}")
            print(f"üìè –î–ª–∏–Ω–∞ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º—Ç–∞: {len(system_prompt)} —Å–∏–º–≤–æ–ª–æ–≤")
            
        except Exception as e:
            system_prompt = "You are a helpful medical assistant."
            lang = 'ru'
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º—Ç–∞: {e}")
        
        # ==========================================
        # –®–ê–ì 8: –°–û–ó–î–ê–ù–ò–ï –§–ò–ù–ê–õ–¨–ù–û–ì–û –ü–†–û–ú–¢–ê
        # ==========================================
        log_step(9, "–°–û–ó–î–ê–ù–ò–ï –§–ò–ù–ê–õ–¨–ù–û–ì–û –ü–†–û–ú–¢–ê")
        
        # ‚úÖ –°–û–ó–î–ê–ï–ú –ü–†–û–ú–¢ –ë–ï–ó last_summary
        user_prompt_parts = [
            "Answer only questions related to the user's health. Do not repeat that you're an AI. Do not ask follow-up questions unless critical.",
            "",
            f"üìå Patient profile:\n{profile_text}",
            "",
            f"üß† Conversation summary:\n{summary_text}",
            "",
            # ‚ùå –£–ë–†–ê–õ–ò: f"üìÑ Recent document interpretations:\n{last_summary}",
            f"üîé Related historical data:\n{chunks_text or '–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞'}",
            "",
            f"Patient: {user_input}"
        ]
        
        final_user_prompt = "\n".join(user_prompt_parts)
        
        print(f"\nüìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–†–û–ú–¢–ê:")
        print(f"   üîß –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º—Ç: {len(system_prompt)} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   üë§ –ü—Ä–æ—Ñ–∏–ª—å: {len(profile_text)} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   üí≠ –°–≤–æ–¥–∫–∞: {len(summary_text)} —Å–∏–º–≤–æ–ª–æ–≤")
        # ‚ùå –£–ë–†–ê–õ–ò: print(f"   üìÑ –ü–æ—Å–ª–µ–¥–Ω–∏–π –¥–æ–∫—É–º–µ–Ω—Ç: {len(last_summary)} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   üîé –ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ: {len(chunks_text)} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   üìè –û–ë–©–ê–Ø –î–õ–ò–ù–ê: {len(final_user_prompt)} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   üéØ –ü—Ä–∏–º–µ—Ä–Ω–æ —Ç–æ–∫–µ–Ω–æ–≤: {len(final_user_prompt) // 4}")
        
        # ‚úÖ –í–û–ó–í–†–ê–©–ê–ï–ú –î–ê–ù–ù–´–ï –ë–ï–ó last_summary
        return {
            "profile_text": profile_text,
            "summary_text": summary_text, 
            # ‚ùå –£–ë–†–ê–õ–ò: "last_summary": last_summary or "–ù–µ—Ç –Ω–µ–¥–∞–≤–Ω–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤",
            "chunks_text": chunks_text or "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞",
            "chunks_found": len(all_chunks),
            "lang": lang if 'lang' in locals() else 'ru',
            "context_text": final_user_prompt  # –î–æ–±–∞–≤–ª—è–µ–º –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        }
        
    except Exception as e:
        log_step(0, "–ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê", f"‚ùå {e}", success=False)
        raise

# üîß –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø –î–õ–Ø –ö–†–ê–¢–ö–û–ì–û –õ–û–ì–ò–†–û–í–ê–ù–ò–Ø (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è)
def log_search_summary(vector_count: int, keyword_count: int, final_count: int):
    """–ö—Ä–∞—Ç–∫–∞—è —Å–≤–æ–¥–∫–∞ –ø–æ–∏—Å–∫–∞ –ë–ï–ó excluded_doc_id"""
    print(f"üß† –ù–∞–π–¥–µ–Ω–æ: {vector_count} –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö + {keyword_count} –∫–ª—é—á–µ–≤—ã—Ö = {final_count} –∏—Ç–æ–≥–æ")