import re
import tiktoken
import chromadb
from chromadb.config import Settings
from chromadb.utils import embedding_functions
from datetime import datetime
from gpt import extract_keywords

client = chromadb.PersistentClient(path="vector_store")
collection = client.get_or_create_collection(name="documents")
embedding_func = embedding_functions.OpenAIEmbeddingFunction(model_name="text-embedding-3-small")

def extract_date_from_text(text: str) -> str:
    match = re.match(r"\[(\d{2})[./](\d{2})[./](\d{4})\]", text.strip())
    if match:
        try:
            date = datetime.strptime(".".join(match.groups()), "%d.%m.%Y")
            return date.strftime("%Y-%m-%d")
        except:
            pass
    return None

async def split_into_chunks(summary, document_id, user_id):
    encoder = tiktoken.encoding_for_model("gpt-4")
    paragraphs = summary.strip().split("\n\n")
    now_str = datetime.now().strftime("%Y-%m-%d")

    chunks = []
    chunk_index = 0

    for para in paragraphs:
        clean_text = para.strip()
        if len(clean_text) < 20:
            continue

        token_count = len(encoder.encode(clean_text))
        #if token_count > 300:
        #   continue  # —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π, –≤–æ–∑–º–æ–∂–Ω–æ –æ—à–∏–±–∫–∞ ‚Äî –ª—É—á—à–µ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å

        found_date = extract_date_from_text(clean_text)
        chunk_date = found_date if found_date else now_str

        # üîπ –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è —ç—Ç–æ–≥–æ –∞–±–∑–∞—Ü–∞
        keywords = await extract_keywords(clean_text)

        chunks.append({
            "chunk_text": clean_text,
            "chunk_index": chunk_index,
            "metadata": {
                "user_id": str(user_id),
                "document_id": str(document_id),
                "confirmed": 1,
                "source": "summary",
                "token_count": token_count,
                "created_at": chunk_date,
                "date_inside": found_date or "",
                "keywords": ", ".join(keywords)  # ‚úÖ —Ç–µ–ø–µ—Ä—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∫–∞–∫ —Å—Ç—Ä–æ–∫–∞
            }
        })
        chunk_index += 1
    
    # ‚ùó –£–¥–∞–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫, –µ—Å–ª–∏ –∏—Ö –±–æ–ª—å—à–µ –æ–¥–Ω–æ–≥–æ
    if len(chunks) > 1:
        chunks = chunks[:-1]

    return chunks

def add_chunks_to_vector_db(chunks):
    """‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: batch insert –≤–º–µ—Å—Ç–æ –ø–æ –æ–¥–Ω–æ–º—É"""
    if not chunks:
        return
    
    documents = []
    ids = []
    metadatas = []
    
    for chunk in chunks:
        doc_id = chunk["metadata"].get("document_id")
        if not doc_id:
            print(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω —á–∞–Ω–∫ –±–µ–∑ document_id:\n{chunk['chunk_text'][:100]}")
            continue
            
        documents.append(chunk["chunk_text"])
        ids.append(f"{chunk['metadata']['document_id']}_{chunk['chunk_index']}")
        metadatas.append(chunk["metadata"])
    
    if documents:
        try:
            collection.add(
                documents=documents,
                ids=ids,
                metadatas=metadatas
            )
            print(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {len(documents)} —á–∞–Ω–∫–æ–≤ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É batch-–æ–º")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ batch insert: {e}")
            # Fallback: –¥–æ–±–∞–≤–ª—è–µ–º –ø–æ –æ–¥–Ω–æ–º—É
            for i, chunk in enumerate(chunks):
                try:
                    collection.add(
                        documents=[documents[i]],
                        ids=[ids[i]],
                        metadatas=[metadatas[i]]
                    )
                except Exception as single_error:
                    print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –¥–æ–±–∞–≤–∏—Ç—å —á–∞–Ω–∫ {i}: {single_error}")

def search_similar_chunks(user_id, query, exclude_doc_id=None, exclude_texts=None, limit=5):
    results = collection.query(
        query_texts=[query],
        n_results=limit + 10,
        where={"$and": [
            {"user_id": str(user_id)},
            {"confirmed": 1}
        ]}
    )
    documents = results.get("documents", [[]])[0]
    metadatas = results.get("metadatas", [[]])[0]

    filtered = []
    for text, meta in zip(documents, metadatas):
        doc_id = meta.get("document_id")
        print(f"üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–µ—Ç–∞: document_id = {doc_id}")

        if exclude_doc_id and str(doc_id) == str(exclude_doc_id):
            print(f"‚õî –ü—Ä–æ–ø—É—â–µ–Ω —á–∞–Ω–∫ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ {doc_id}")
            continue
        if exclude_texts and text.strip() in exclude_texts:
            print(f"‚õî –ü—Ä–æ–ø—É—â–µ–Ω —á–∞–Ω–∫ –ø–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—é —Ç–µ–∫—Å—Ç–∞")
            continue

        filtered.append((text.strip(), doc_id))
        if len(filtered) == limit:
            break

    print("\nüß† –ò—Ç–æ–≥–æ–≤—ã–µ —á–∞–Ω–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–π–¥—É—Ç –≤ –ø—Ä–æ–º—Ç:")
    for i, (text, doc_id) in enumerate(filtered):
        print(f"\nüìÑ CHUNK {i+1} (document_id = {doc_id}):\n{text.strip()[:500]}")

    return [text for text, _ in filtered]

async def keyword_search_chunks(user_id: int, user_question: str, exclude_doc_id=None, exclude_texts=None, limit=5):
    """‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: –Ω–µ –∑–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å—Ä–∞–∑—É"""
    from gpt import extract_keywords
    keywords = await extract_keywords(user_question)
    print(f"üîé –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞: {keywords}")

    # –ü—Ä–∏–≤–æ–¥–∏–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É –∏ —É–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã
    query_keywords = [k.strip().lower() for k in keywords]

    # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –∏—Å–ø–æ–ª—å–∑—É–µ–º where –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ user_id
    try:
        raw = collection.get(
            where={"user_id": str(user_id), "confirmed": 1},  # –§–∏–ª—å—Ç—Ä—É–µ–º —Å—Ä–∞–∑—É
            include=["documents", "metadatas"]
        )
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–µ: {e}")
        return []

    documents = raw.get("documents", [])
    metadatas = raw.get("metadatas", [])

    results = []
    seen_docs = set()

    for text, meta in zip(documents, metadatas):
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ (–Ω–∞ —Å–ª—É—á–∞–π –µ—Å–ª–∏ where –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª)
        if str(meta.get("user_id")) != str(user_id):
            continue
        if meta.get("confirmed") != 1:
            continue
        if exclude_doc_id and str(meta.get("document_id")) == str(exclude_doc_id):
            continue
        if exclude_texts and text.strip() in exclude_texts:
            continue

        raw_keywords = meta.get("keywords", "")
        if isinstance(raw_keywords, str):
            chunk_keywords = [kw.strip().lower() for kw in raw_keywords.split(",")]
        elif isinstance(raw_keywords, list):
            chunk_keywords = [kw.strip().lower() for kw in raw_keywords]
        else:
            chunk_keywords = []

        if any(qk in chunk_keywords for qk in query_keywords):
            doc_id = meta.get("document_id")
            if doc_id not in seen_docs:
                results.append((text.strip(), doc_id))
                seen_docs.add(doc_id)
        if len(results) >= limit:
            break

    print(f"üìö Keyword-–ø–æ–∏—Å–∫ –¥–∞–ª {len(results)} —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π.")
    return [text for text, _ in results]