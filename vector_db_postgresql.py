# vector_db_postgresql.py - –ó–∞–º–µ–Ω–∞ ChromaDB –Ω–∞ PostgreSQL + pgvector

import re
import tiktoken
import asyncpg
import numpy as np
import json
from typing import List, Dict, Optional, Tuple
from openai import OpenAI
import os
from datetime import datetime
import logging

logger = logging.getLogger(__name__)

def get_openai_client():
    """–ü–æ–ª—É—á–∞–µ—Ç OpenAI –∫–ª–∏–µ–Ω—Ç —Å –ª–µ–Ω–∏–≤–æ–π –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–µ–π"""
    from openai import OpenAI
    return OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

class PostgreSQLVectorDB:
    """
    –í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ PostgreSQL —Å pgvector
    –ó–∞–º–µ–Ω–∞ ChromaDB –¥–ª—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–≥–æ –±–æ—Ç–∞
    """
    
    def __init__(self, db_pool):
        self.db_pool = db_pool
    
    async def initialize_vector_tables(self):
        """–°–æ–∑–¥–∞–µ—Ç —Ç–∞–±–ª–∏—Ü—ã –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        
        create_tables_sql = """
        -- üîå –í–∫–ª—é—á–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ pgvector
        CREATE EXTENSION IF NOT EXISTS vector;
        
        -- üìä –¢–ê–ë–õ–ò–¶–ê –í–ï–ö–¢–û–†–û–í –î–û–ö–£–ú–ï–ù–¢–û–í
        CREATE TABLE IF NOT EXISTS document_vectors (
            id SERIAL PRIMARY KEY,
            document_id INTEGER REFERENCES documents(id) ON DELETE CASCADE,
            user_id BIGINT REFERENCES users(user_id) ON DELETE CASCADE,
            chunk_index INTEGER NOT NULL,
            chunk_text TEXT NOT NULL,
            embedding vector(1536),  -- OpenAI text-embedding-3-small —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
            metadata JSONB DEFAULT '{}',
            keywords TEXT DEFAULT '',
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            
            -- üîç –ò–ù–î–ï–ö–°–´ –î–õ–Ø –ë–´–°–¢–†–û–ì–û –ü–û–ò–°–ö–ê
            CONSTRAINT unique_chunk UNIQUE(document_id, chunk_index)
        );
        
        -- üìà –ò–ù–î–ï–ö–°–´ –î–õ–Ø –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò
        CREATE INDEX IF NOT EXISTS idx_document_vectors_user_id ON document_vectors(user_id);
        CREATE INDEX IF NOT EXISTS idx_document_vectors_document_id ON document_vectors(document_id);
        CREATE INDEX IF NOT EXISTS idx_document_vectors_embedding ON document_vectors USING ivfflat (embedding vector_cosine_ops);
        CREATE INDEX IF NOT EXISTS idx_document_vectors_keywords ON document_vectors USING gin(to_tsvector('russian', keywords));
        """
        
        conn = await self.db_pool.acquire()
        try:
            await conn.execute(create_tables_sql)
            logger.info("‚úÖ –í–µ–∫—Ç–æ—Ä–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã PostgreSQL —Å–æ–∑–¥–∞–Ω—ã")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü: {e}")
            raise
        finally:
            await self.db_pool.release(conn)
    
    async def get_embedding(self, text: str) -> List[float]:
        """–ü–æ–ª—É—á–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥ –æ—Ç OpenAI"""
        try:
            # ‚úÖ –°–û–ó–î–ê–ï–ú –ö–õ–ò–ï–ù–¢ –¢–û–õ–¨–ö–û –ö–û–ì–î–ê –ù–£–ñ–ï–ù:
            client = get_openai_client()
            
            response = await client.embeddings.create(
                model="text-embedding-3-small",
                input=text.replace("\n", " ")[:8000]
            )
            return response.data[0].embedding
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {e}")
            raise
    
    async def add_document_chunks(self, document_id: int, user_id: int, chunks: List[Dict]) -> bool:
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç —á–∞–Ω–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É
        
        Args:
            document_id: ID –¥–æ–∫—É–º–µ–Ω—Ç–∞
            user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è  
            chunks: –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ —Å —Ç–µ–∫—Å—Ç–æ–º –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        conn = await self.db_pool.acquire()
        try:
            # üóëÔ∏è –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –≤–µ–∫—Ç–æ—Ä—ã —ç—Ç–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            await conn.execute(
                "DELETE FROM document_vectors WHERE document_id = $1",
                document_id
            )
            
            # ‚ûï –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –≤–µ–∫—Ç–æ—Ä—ã
            for chunk in chunks:
                # üß† –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
                embedding = await self.get_embedding(chunk['chunk_text'])
                
                # üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –±–∞–∑—É
                await conn.execute("""
                    INSERT INTO document_vectors 
                    (document_id, user_id, chunk_index, chunk_text, embedding, metadata, keywords)
                    VALUES ($1, $2, $3, $4, $5, $6, $7)
                """, 
                    document_id,
                    user_id,
                    chunk['chunk_index'],
                    chunk['chunk_text'],
                    embedding,
                    json.dumps(chunk['metadata']),
                    chunk['metadata'].get('keywords', '')
                )
            
            logger.info(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {len(chunks)} –≤–µ–∫—Ç–æ—Ä–æ–≤ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ {document_id}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–æ–≤: {e}")
            return False
        finally:
            await self.db_pool.release(conn)
    
    async def search_similar_chunks(self, user_id: int, query: str, limit: int = 5) -> List[Dict]:
        """
        –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ —á–∞–Ω–∫–∞–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        
        Args:
            user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            limit: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
        """
        conn = await self.db_pool.acquire()
        try:
            # üß† –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞
            query_embedding = await self.get_embedding(query)
            
            # üîç –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫
            results = await conn.fetch("""
                SELECT 
                    dv.chunk_text,
                    dv.metadata,
                    dv.keywords,
                    d.title as document_title,
                    d.uploaded_at,
                    (dv.embedding <=> $1::vector) as distance
                FROM document_vectors dv
                JOIN documents d ON d.id = dv.document_id
                WHERE dv.user_id = $2
                ORDER BY dv.embedding <=> $1::vector
                LIMIT $3
            """, query_embedding, user_id, limit)
            
            # üìä –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            chunks = []
            for row in results:
                chunks.append({
                    "chunk_text": row['chunk_text'],
                    "metadata": json.loads(row['metadata']),
                    "keywords": row['keywords'],
                    "document_title": row['document_title'],
                    "uploaded_at": row['uploaded_at'],
                    "similarity": 1 - row['distance']  # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º distance –≤ similarity
                })
            
            logger.info(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(chunks)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}")
            return chunks
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")
            return []
        finally:
            await self.db_pool.release(conn)
    
    async def keyword_search_chunks(self, user_id: int, keywords: str, limit: int = 5) -> List[Dict]:
        """
        –¢–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        
        Args:
            user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            keywords: –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞
            limit: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        """
        conn = await self.db_pool.acquire()
        try:
            # üîç –ü–æ–ª–Ω–æ—Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ –ø–æ PostgreSQL
            results = await conn.fetch("""
                SELECT 
                    dv.chunk_text,
                    dv.metadata,
                    dv.keywords,
                    d.title as document_title,
                    d.uploaded_at,
                    ts_rank(to_tsvector('russian', dv.chunk_text || ' ' || dv.keywords), 
                           plainto_tsquery('russian', $1)) as rank
                FROM document_vectors dv
                JOIN documents d ON d.id = dv.document_id
                WHERE dv.user_id = $2
                  AND (to_tsvector('russian', dv.chunk_text || ' ' || dv.keywords) @@ 
                       plainto_tsquery('russian', $1))
                ORDER BY rank DESC
                LIMIT $3
            """, keywords, user_id, limit)
            
            # üìä –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            chunks = []
            for row in results:
                chunks.append({
                    "chunk_text": row['chunk_text'],
                    "metadata": json.loads(row['metadata']),
                    "keywords": row['keywords'],
                    "document_title": row['document_title'],
                    "uploaded_at": row['uploaded_at'],
                    "rank": float(row['rank'])
                })
            
            logger.info(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}")
            return chunks
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º: {e}")
            return []
        finally:
            await self.db_pool.release(conn)
    
    async def delete_document_vectors(self, document_id: int):
        """–£–¥–∞–ª—è–µ—Ç –≤—Å–µ –≤–µ–∫—Ç–æ—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        conn = await self.db_pool.acquire()
        try:
            result = await conn.execute(
                "DELETE FROM document_vectors WHERE document_id = $1",
                document_id
            )
            deleted_count = int(result.split()[-1])  # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–¥–∞–ª–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–æ–∫
            logger.info(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–æ {deleted_count} –≤–µ–∫—Ç–æ—Ä–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞ {document_id}")
            return True
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞ {document_id}: {e}")
            return False
        finally:
            await self.db_pool.release(conn)
    
    async def delete_user_vectors(self, user_id: int):
        """–£–¥–∞–ª—è–µ—Ç –≤—Å–µ –≤–µ–∫—Ç–æ—Ä—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        conn = await self.db_pool.acquire()
        try:
            result = await conn.execute(
                "DELETE FROM document_vectors WHERE user_id = $1",
                user_id
            )
            deleted_count = int(result.split()[-1])
            logger.info(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–æ {deleted_count} –≤–µ–∫—Ç–æ—Ä–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}")
            return True
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}: {e}")
            return False
        finally:
            await self.db_pool.release(conn)
    
    async def get_vector_stats(self) -> Dict:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã"""
        conn = await self.db_pool.acquire()
        try:
            stats = await conn.fetchrow("""
                SELECT 
                    COUNT(*) as total_vectors,
                    COUNT(DISTINCT user_id) as unique_users,
                    COUNT(DISTINCT document_id) as unique_documents,
                    AVG(length(chunk_text)) as avg_chunk_length
                FROM document_vectors
            """)
            
            return {
                "total_vectors": stats['total_vectors'],
                "unique_users": stats['unique_users'], 
                "unique_documents": stats['unique_documents'],
                "avg_chunk_length": round(stats['avg_chunk_length'], 1) if stats['avg_chunk_length'] else 0
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
            return {"total_vectors": 0, "unique_users": 0, "unique_documents": 0}
        finally:
            await self.db_pool.release(conn)

# üåê –ì–õ–û–ë–ê–õ–¨–ù–´–ô –≠–ö–ó–ï–ú–ü–õ–Ø–† (–±—É–¥–µ—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –≤ main.py)
vector_db: Optional[PostgreSQLVectorDB] = None

async def initialize_vector_db(db_pool):
    from db_postgresql import db_pool  # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –≤–Ω—É—Ç—Ä–∏ —Ñ—É–Ω–∫—Ü–∏–∏
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
    global vector_db
    vector_db = PostgreSQLVectorDB(db_pool)
    await vector_db.initialize_vector_tables()
    logger.info("‚úÖ PostgreSQL Vector DB –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")

# üîÑ –§–£–ù–ö–¶–ò–ò –°–û–í–ú–ï–°–¢–ò–ú–û–°–¢–ò (—á—Ç–æ–±—ã –Ω–µ –º–µ–Ω—è—Ç—å –≤–µ—Å—å –∫–æ–¥)
async def add_chunks_to_vector_db(document_id: int, user_id: int, chunks: List[Dict]):
    """–î–æ–±–∞–≤–ª—è–µ—Ç —á–∞–Ω–∫–∏ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å ChromaDB)"""
    if vector_db:
        return await vector_db.add_document_chunks(document_id, user_id, chunks)
    return False

async def search_similar_chunks(user_id: int, query: str, limit: int = 5) -> List[Dict]:
    """–ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö —á–∞–Ω–∫–æ–≤ (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å ChromaDB)"""
    if vector_db:
        return await vector_db.search_similar_chunks(user_id, query, limit)
    return []

async def keyword_search_chunks(user_id: int, keywords: str, limit: int = 5) -> List[Dict]:
    """–ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å ChromaDB)"""
    if vector_db:
        return await vector_db.keyword_search_chunks(user_id, keywords, limit)
    return []

async def delete_document_from_vector_db(document_id: int):
    """–£–¥–∞–ª—è–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –∏–∑ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å ChromaDB)"""
    if vector_db:
        return await vector_db.delete_document_vectors(document_id)
    return False

async def extract_date_from_text(text: str) -> str:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞—Ç—É –∏–∑ —Ç–µ–∫—Å—Ç–∞ (–ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–æ –∏–∑ vector_utils.py)"""
    match = re.match(r"\[(\d{2})[./](\d{2})[./](\d{4})\]", text.strip())
    if match:
        try:
            date = datetime.strptime(".".join(match.groups()), "%d.%m.%Y")
            return date.strftime("%Y-%m-%d")
        except:
            pass
    return None

async def split_into_chunks(summary: str, document_id: int, user_id: int) -> List[Dict]:
    """
    –†–∞–∑–±–∏–≤–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞ —á–∞–Ω–∫–∏ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
    –ü–µ—Ä–µ–Ω–µ—Å–µ–Ω–æ –∏–∑ vector_utils.py –∏ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è PostgreSQL
    """
    from gpt import extract_keywords  # –ò–º–ø–æ—Ä—Ç –≤–Ω—É—Ç—Ä–∏ —Ñ—É–Ω–∫—Ü–∏–∏
    
    encoder = tiktoken.encoding_for_model("gpt-4")
    paragraphs = summary.strip().split("\n\n")
    now_str = datetime.now().strftime("%Y-%m-%d")

    chunks = []
    chunk_index = 0

    for para in paragraphs:
        clean_text = para.strip()
        if len(clean_text) < 20:
            continue

        token_count = len(encoder.encode(clean_text))
        
        found_date = await extract_date_from_text(clean_text)
        chunk_date = found_date if found_date else now_str

        # üîπ –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è —ç—Ç–æ–≥–æ –∞–±–∑–∞—Ü–∞
        keywords = await extract_keywords(clean_text)

        chunks.append({
            "chunk_text": clean_text,
            "chunk_index": chunk_index,
            "metadata": {
                "user_id": str(user_id),
                "document_id": str(document_id),
                "confirmed": 1,
                "source": "summary",
                "token_count": token_count,
                "created_at": chunk_date,
                "date_inside": found_date or "",
                "keywords": ", ".join(keywords)
            }
        })
        chunk_index += 1
    
    # ‚ùó –£–¥–∞–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫, –µ—Å–ª–∏ –∏—Ö –±–æ–ª—å—à–µ –æ–¥–Ω–æ–≥–æ (–ª–æ–≥–∏–∫–∞ –∏–∑ vector_utils)
    if len(chunks) > 1:
        chunks = chunks[:-1]

    return chunks

# ‚úÖ –û–ë–ù–û–í–õ–Ø–ï–ú —Ñ—É–Ω–∫—Ü–∏—é add_chunks_to_vector_db –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏:

async def add_chunks_to_vector_db(document_id: int, user_id: int, chunks: List[Dict]):
    """
    –î–æ–±–∞–≤–ª—è–µ—Ç —á–∞–Ω–∫–∏ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å ChromaDB)
    –¢–µ–ø–µ—Ä—å —ç—Ç–æ –æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è PostgreSQL —Ñ—É–Ω–∫—Ü–∏–∏
    """
    if vector_db:
        return await vector_db.add_document_chunks(document_id, user_id, chunks)
    return False

# ‚úÖ –î–û–ë–ê–í–õ–Ø–ï–ú —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –ø–æ–ª–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å vector_utils.py:

async def delete_all_chunks_by_user(user_id: int):
    """–£–¥–∞–ª—è–µ—Ç –≤—Å–µ –≤–µ–∫—Ç–æ—Ä—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å vector_db.py)"""
    if vector_db:
        return await vector_db.delete_user_vectors(user_id)
    return False

async def mark_chunks_unconfirmed(document_id: int):
    """
    –ü–æ–º–µ—á–∞–µ—Ç —á–∞–Ω–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∫–∞–∫ –Ω–µ–ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–Ω—ã–µ
    (–≤ PostgreSQL –≤–µ—Ä—Å–∏–∏ –º–æ–∂–Ω–æ –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤—ã–≤–∞—Ç—å –∏–ª–∏ —Å–¥–µ–ª–∞—Ç—å –∑–∞–≥–ª—É—à–∫—É)
    """
    # –í PostgreSQL –≤–µ—Ä—Å–∏–∏ —ç—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –º–æ–∂–µ—Ç –±—ã—Ç—å –∑–∞–≥–ª—É—à–∫–æ–π
    # —Ç–∞–∫ –∫–∞–∫ —É –Ω–∞—Å –Ω–µ—Ç –ø–æ–ª—è "confirmed" –∏–ª–∏ –æ–Ω–æ –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ
    logger.info(f"mark_chunks_unconfirmed({document_id}) - –∑–∞–≥–ª—É—à–∫–∞ –¥–ª—è PostgreSQL")
    return True

async def get_collection_stats():
    """–ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å vector_db.py)"""
    if vector_db:
        return await vector_db.get_vector_stats()
    return {"total_documents": 0, "status": "error"}

# ‚úÖ –§–£–ù–ö–¶–ò–ò –î–õ–Ø –†–ê–ë–û–¢–´ –° –≠–ú–ë–ï–î–î–ò–ù–ì–ê–ú–ò (–µ—Å–ª–∏ –Ω—É–∂–Ω—ã):

def validate_embedding_dimensions(embedding: List[float]) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞"""
    return len(embedding) == 1536  # OpenAI text-embedding-3-small

async def batch_get_embeddings(texts: List[str]) -> List[List[float]]:
    """–ü–æ–ª—É—á–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ (batch –æ–±—Ä–∞–±–æ—Ç–∫–∞)"""
    embeddings = []
    for text in texts:
        if vector_db:
            embedding = await vector_db.get_embedding(text)
            embeddings.append(embedding)
        else:
            embeddings.append([0.0] * 1536)  # –ó–∞–≥–ª—É—à–∫–∞
    return embeddings

# üåê –ì–õ–û–ë–ê–õ–¨–ù–´–ô –î–û–°–¢–£–ü –ö –ë–î –ü–£–õ–£
async def initialize_vector_db(db_pool=None):
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
    global vector_db
    
    # –ü–æ–ª—É—á–∞–µ–º –ø—É–ª –∏–∑ db_postgresql –µ—Å–ª–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω
    if db_pool is None:
        from db_postgresql import db_pool as main_db_pool
        db_pool = main_db_pool
    
    vector_db = PostgreSQLVectorDB(db_pool)
    await vector_db.initialize_vector_tables()
    logger.info("‚úÖ PostgreSQL Vector DB –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")

# üîÑ –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò –°–û–í–ú–ï–°–¢–ò–ú–û–°–¢–ò

async def delete_all_chunks_by_user(user_id: int):
    """–£–¥–∞–ª—è–µ—Ç –≤—Å–µ –≤–µ–∫—Ç–æ—Ä—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å vector_db.py)"""
    if vector_db:
        return await vector_db.delete_user_vectors(user_id)
    return False

async def mark_chunks_unconfirmed(document_id: int):
    """
    –ü–æ–º–µ—á–∞–µ—Ç —á–∞–Ω–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∫–∞–∫ –Ω–µ–ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–Ω—ã–µ
    (–≤ PostgreSQL –≤–µ—Ä—Å–∏–∏ –º–æ–∂–Ω–æ –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤—ã–≤–∞—Ç—å –∏–ª–∏ —Å–¥–µ–ª–∞—Ç—å –∑–∞–≥–ª—É—à–∫—É)
    """
    # –í PostgreSQL –≤–µ—Ä—Å–∏–∏ —ç—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –º–æ–∂–µ—Ç –±—ã—Ç—å –∑–∞–≥–ª—É—à–∫–æ–π
    # —Ç–∞–∫ –∫–∞–∫ —É –Ω–∞—Å –Ω–µ—Ç –ø–æ–ª—è "confirmed" –∏–ª–∏ –æ–Ω–æ –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ
    logger.info(f"mark_chunks_unconfirmed({document_id}) - –∑–∞–≥–ª—É—à–∫–∞ –¥–ª—è PostgreSQL")
    return True

async def get_collection_stats():
    """–ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å vector_db.py)"""
    if vector_db:
        return await vector_db.get_vector_stats()
    return {"total_documents": 0, "status": "error"}

# ‚úÖ –§–£–ù–ö–¶–ò–ò –î–õ–Ø –†–ê–ë–û–¢–´ –° –≠–ú–ë–ï–î–î–ò–ù–ì–ê–ú–ò (–µ—Å–ª–∏ –Ω—É–∂–Ω—ã):

def validate_embedding_dimensions(embedding: List[float]) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞"""
    return len(embedding) == 1536  # OpenAI text-embedding-3-small

async def batch_get_embeddings(texts: List[str]) -> List[List[float]]:
    """–ü–æ–ª—É—á–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ (batch –æ–±—Ä–∞–±–æ—Ç–∫–∞)"""
    embeddings = []
    for text in texts:
        if vector_db:
            embedding = await vector_db.get_embedding(text)
            embeddings.append(embedding)
        else:
            embeddings.append([0.0] * 1536)  # –ó–∞–≥–ª—É—à–∫–∞
    return embeddings

# üõ†Ô∏è –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø –í –°–£–©–ï–°–¢–í–£–Æ–©–ò–• –§–£–ù–ö–¶–ò–Ø–•

# –ò—Å–ø—Ä–∞–≤–ª—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é split_into_chunks –µ—Å–ª–∏ –µ—Å—Ç—å –ø—Ä–æ–±–ª–µ–º—ã —Å extract_keywords
async def split_into_chunks(summary: str, document_id: int, user_id: int) -> List[Dict]:
    """
    –†–∞–∑–±–∏–≤–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞ —á–∞–Ω–∫–∏ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
    –ü–µ—Ä–µ–Ω–µ—Å–µ–Ω–æ –∏–∑ vector_utils.py –∏ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è PostgreSQL
    """
    import tiktoken
    
    encoder = tiktoken.encoding_for_model("gpt-4")
    paragraphs = summary.strip().split("\n\n")
    now_str = datetime.now().strftime("%Y-%m-%d")

    chunks = []
    chunk_index = 0

    for para in paragraphs:
        clean_text = para.strip()
        if len(clean_text) < 20:
            continue

        token_count = len(encoder.encode(clean_text))
        
        found_date = await extract_date_from_text(clean_text)
        chunk_date = found_date if found_date else now_str

        # üîπ –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–±–µ–∑–æ–ø–∞—Å–Ω–æ)
        try:
            from gpt import extract_keywords
            keywords = await extract_keywords(clean_text)
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {e}")
            keywords = []  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –ø—Ä–∏ –æ—à–∏–±–∫–µ

        chunks.append({
            "chunk_text": clean_text,
            "chunk_index": chunk_index,
            "metadata": {
                "user_id": str(user_id),
                "document_id": str(document_id),
                "confirmed": 1,
                "source": "summary",
                "token_count": token_count,
                "created_at": chunk_date,
                "date_inside": found_date or "",
                "keywords": ", ".join(keywords) if keywords else ""
            }
        })
        chunk_index += 1
    
    # ‚ùó –£–¥–∞–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫, –µ—Å–ª–∏ –∏—Ö –±–æ–ª—å—à–µ –æ–¥–Ω–æ–≥–æ (–ª–æ–≥–∏–∫–∞ –∏–∑ vector_utils)
    if len(chunks) > 1:
        chunks = chunks[:-1]

    return chunks

# üîß –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï –§–£–ù–ö–¶–ò–ò –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–ò

async def initialize_vector_db_safe():
    """–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã —Å –ø—Ä–æ–≤–µ—Ä–∫–∞–º–∏"""
    try:
        from db_postgresql import db_pool
        
        if db_pool is None:
            logger.error("‚ùå db_pool –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω!")
            return False
            
        await initialize_vector_db(db_pool)
        return True
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã: {e}")
        return False