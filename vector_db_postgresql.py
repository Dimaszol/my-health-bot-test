# vector_db_postgresql.py - –ó–∞–º–µ–Ω–∞ ChromaDB –Ω–∞ PostgreSQL + pgvector

import re
import tiktoken
import asyncpg
import numpy as np
import json
from typing import List, Dict, Optional, Tuple
from openai import OpenAI
import os
from datetime import datetime
import logging

logger = logging.getLogger(__name__)

def get_openai_client():
    """–ü–æ–ª—É—á–∞–µ—Ç OpenAI –∫–ª–∏–µ–Ω—Ç —Å –ª–µ–Ω–∏–≤–æ–π –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–µ–π"""
    from openai import OpenAI
    return OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

class PostgreSQLVectorDB:
    """
    –í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ PostgreSQL —Å pgvector
    –ó–∞–º–µ–Ω–∞ ChromaDB –¥–ª—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–≥–æ –±–æ—Ç–∞
    """
    
    def __init__(self, db_pool):
        self.db_pool = db_pool
    
    async def initialize_vector_tables(self):
        """–°–æ–∑–¥–∞–µ—Ç —Ç–∞–±–ª–∏—Ü—ã –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        
        create_tables_sql = """
        -- üîå –í–∫–ª—é—á–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ pgvector
        CREATE EXTENSION IF NOT EXISTS vector;
        
        -- üìä –¢–ê–ë–õ–ò–¶–ê –í–ï–ö–¢–û–†–û–í –î–û–ö–£–ú–ï–ù–¢–û–í
        CREATE TABLE IF NOT EXISTS document_vectors (
            id SERIAL PRIMARY KEY,
            document_id INTEGER REFERENCES documents(id) ON DELETE CASCADE,
            user_id BIGINT REFERENCES users(user_id) ON DELETE CASCADE,
            chunk_index INTEGER NOT NULL,
            chunk_text TEXT NOT NULL,
            embedding vector(1536),  -- OpenAI text-embedding-3-small —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
            metadata JSONB DEFAULT '{}',
            keywords TEXT DEFAULT '',
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            
            -- üîç –ò–ù–î–ï–ö–°–´ –î–õ–Ø –ë–´–°–¢–†–û–ì–û –ü–û–ò–°–ö–ê
            CONSTRAINT unique_chunk UNIQUE(document_id, chunk_index)
        );
        
        -- üìà –ò–ù–î–ï–ö–°–´ –î–õ–Ø –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò
        CREATE INDEX IF NOT EXISTS idx_document_vectors_user_id ON document_vectors(user_id);
        CREATE INDEX IF NOT EXISTS idx_document_vectors_document_id ON document_vectors(document_id);
        CREATE INDEX IF NOT EXISTS idx_document_vectors_embedding ON document_vectors USING ivfflat (embedding vector_cosine_ops);
        CREATE INDEX IF NOT EXISTS idx_document_vectors_keywords ON document_vectors USING gin(to_tsvector('russian', keywords));
        """
        
        conn = await self.db_pool.acquire()
        try:
            await conn.execute(create_tables_sql)
            logger.info("‚úÖ –í–µ–∫—Ç–æ—Ä–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã PostgreSQL —Å–æ–∑–¥–∞–Ω—ã")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü: {e}")
            raise
        finally:
            await self.db_pool.release(conn)
    
    async def get_embedding(self, text: str) -> List[float]:
        """–ü–æ–ª—É—á–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥ –æ—Ç OpenAI"""
        try:
            # ‚úÖ –°–û–ó–î–ê–ï–ú –ö–õ–ò–ï–ù–¢ –¢–û–õ–¨–ö–û –ö–û–ì–î–ê –ù–£–ñ–ï–ù:
            client = get_openai_client()
            
            response = client.embeddings.create(
                model="text-embedding-3-small",
                input=text.replace("\n", " ")[:8000]
            )
            return response.data[0].embedding
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {e}")
            raise
    
    async def add_document_chunks(self, document_id: int, user_id: int, chunks: List[Dict]) -> bool:
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç —á–∞–Ω–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É
        
        Args:
            document_id: ID –¥–æ–∫—É–º–µ–Ω—Ç–∞
            user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è  
            chunks: –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ —Å —Ç–µ–∫—Å—Ç–æ–º –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        conn = await self.db_pool.acquire()
        try:
            # üóëÔ∏è –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –≤–µ–∫—Ç–æ—Ä—ã —ç—Ç–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            await conn.execute(
                "DELETE FROM document_vectors WHERE document_id = $1",
                document_id
            )
            
            # ‚ûï –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –≤–µ–∫—Ç–æ—Ä—ã
            for chunk in chunks:
                # üß† –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
                embedding = await self.get_embedding(chunk['chunk_text'])
                
                # üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –±–∞–∑—É
                await conn.execute("""
                    INSERT INTO document_vectors 
                    (document_id, user_id, chunk_index, chunk_text, embedding, metadata, keywords)
                    VALUES ($1, $2, $3, $4, $5, $6, $7)
                """, 
                    document_id,
                    user_id,
                    chunk['chunk_index'],
                    chunk['chunk_text'],
                    f"[{','.join(map(str, embedding))}]",
                    json.dumps(chunk['metadata']),
                    chunk['metadata'].get('keywords', '')
                )
            
            logger.info(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {len(chunks)} –≤–µ–∫—Ç–æ—Ä–æ–≤ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ {document_id}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–æ–≤: {e}")
            return False
        finally:
            await self.db_pool.release(conn)
    
    async def search_similar_chunks(self, user_id: int, query: str, limit: int = 5, similarity_threshold: float = 0.3) -> List[Dict]:
        """
        –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ –ø–æ—Ä–æ–≥—É —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        
        Args:
            user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            similarity_threshold: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ —Å—Ö–æ–¥—Å—Ç–≤–∞ (0.0-1.0)
                - 0.85+ = –æ—á–µ–Ω—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                - 0.7+ = —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã  
                - 0.5+ = —É–º–µ—Ä–µ–Ω–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ
                - <0.5 = —Å–ª–∞–±–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ (–ª—É—á—à–µ –∏—Å–∫–ª—é—á–∏—Ç—å)
                
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤, –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ similarity
        """
        conn = await self.db_pool.acquire()
        try:
            # üß† –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞
            query_embedding = await self.get_embedding(query)
            
            # üîß –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º list –≤ —Å—Ç—Ä–æ–∫—É –¥–ª—è PostgreSQL
            if isinstance(query_embedding, list):
                embedding_str = '[' + ','.join(map(str, query_embedding)) + ']'
            else:
                embedding_str = query_embedding
                                    
            # üîç –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ threshold
            # –ò—â–µ–º –±–æ–ª—å—à–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            search_limit = min(limit * 3, 20)  # –ù–µ –±–æ–ª—å—à–µ 20 –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            
            results = await conn.fetch("""
                WITH ranked_chunks AS (
                    SELECT 
                        dv.chunk_text,
                        dv.metadata,
                        dv.keywords,
                        d.title as document_title,
                        d.uploaded_at,
                        (dv.embedding <=> $1::vector) as distance,
                        (1 - (dv.embedding <=> $1::vector)) as similarity,
                        -- üìä –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
                        CASE 
                            WHEN d.uploaded_at > NOW() - INTERVAL '30 days' THEN 0.1
                            WHEN d.uploaded_at > NOW() - INTERVAL '90 days' THEN 0.05
                            ELSE 0.0
                        END as recency_boost,
                        LENGTH(dv.chunk_text) as chunk_length
                    FROM document_vectors dv
                    JOIN documents d ON d.id = dv.document_id
                    WHERE dv.user_id = $2
                    ORDER BY dv.embedding <=> $1::vector
                    LIMIT $3
                )
                SELECT 
                    chunk_text,
                    metadata,
                    keywords,
                    document_title,
                    uploaded_at,
                    distance,
                    similarity,
                    (similarity + recency_boost) as final_score,
                    chunk_length
                FROM ranked_chunks
                WHERE similarity >= $4  -- üéØ –§–ò–õ–¨–¢–†–ê–¶–ò–Ø –ü–û THRESHOLD
                ORDER BY final_score DESC, similarity DESC
                LIMIT $5
            """, embedding_str, user_id, search_limit, similarity_threshold, limit)
            
            # üìä –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –ø–æ–¥—Ä–æ–±–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
            chunks = []
            for row in results:
                # –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ metadata
                try:
                    metadata = json.loads(row['metadata']) if row['metadata'] else {}
                except (json.JSONDecodeError, TypeError):
                    metadata = {}
                
                chunk_data = {
                    "chunk_text": row['chunk_text'],
                    "metadata": metadata,
                    "keywords": row['keywords'],
                    "document_title": row['document_title'],
                    "uploaded_at": row['uploaded_at'],
                    "similarity": round(float(row['similarity']), 3),
                    "final_score": round(float(row['final_score']), 3),
                    "chunk_length": row['chunk_length']
                }
                chunks.append(chunk_data)
            
            # üìà –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            if chunks:
                best_similarity = chunks[0]['similarity']
                worst_similarity = chunks[-1]['similarity']
                logger.info(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(chunks)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}")
                                
                # üö® –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ –Ω–∏–∑–∫–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
                if best_similarity < 0.6:
                    logger.warning(f"‚ö†Ô∏è –ù–∏–∑–∫–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∑–∞–ø—Ä–æ—Å–∞: '{query[:50]}...' (max={best_similarity:.3f})")
            else:
                logger.info(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: '{query[:50]}...' (threshold={similarity_threshold})")
                
            return chunks
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")
            return []
        finally:
            await self.db_pool.release(conn)
    
    async def keyword_search_chunks(self, user_id: int, keywords: str, limit: int = 5) -> List[Dict]:
        """
        üîç –£–õ–£–ß–®–ï–ù–ù–´–ô –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º —Å —Ç–æ—á–Ω—ã–º –ø–æ–¥—Å—á–µ—Ç–æ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
        
        –¢–µ–ø–µ—Ä—å —Ç–æ—á–Ω–æ —Å—á–∏—Ç–∞–µ—Ç —Å–æ–≤–ø–∞–≤—à–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞:
        - "–£–ó–ò –ø–µ—á–µ–Ω–∏" –Ω–∞–π–¥–µ—Ç —á–∞–Ω–∫–∏ —Å –æ–±–æ–∏–º–∏ —Å–ª–æ–≤–∞–º–∏ –≤—ã—à–µ —á–µ–º —Å –æ–¥–Ω–∏–º
        - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ä–∞–Ω–∂–∏—Ä—É–µ—Ç –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
        """
        conn = await self.db_pool.acquire()
        try:
            logger.info(f"üîç –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º: '{keywords}' –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}")
            
            # üîπ –†–∞–∑–±–∏–≤–∞–µ–º –∏ –æ—á–∏—â–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            keyword_list = [k.strip().lower() for k in keywords.split(',') if k.strip()]
            
            if not keyword_list:
                logger.info("‚ùå –ü—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤")
                return []
            
            print(f"üîë –ò—â–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –¥–ª—è: {keyword_list}")
            
            # üîß –°–æ–∑–¥–∞–µ–º SQL —Å —Ç–æ—á–Ω—ã–º –ø–æ–¥—Å—á–µ—Ç–æ–º –∫–∞–∂–¥–æ–≥–æ –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞
            params = [user_id]
            param_index = 2
            
            # –°–æ–∑–¥–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–µ —É—Å–ª–æ–≤–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞
            match_conditions = []
            
            for keyword in keyword_list:
                match_conditions.append(f"dv.keywords ILIKE ${param_index}")
                params.append(f'%{keyword}%')
                param_index += 1
            
            # –û–±—â–µ–µ —É—Å–ª–æ–≤–∏–µ –ø–æ–∏—Å–∫–∞ (–ª—é–±–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ)
            where_clause = " OR ".join(match_conditions)
            
            # –ü–æ–¥—Å—á–µ—Ç –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
            total_matches = " + ".join([f"CASE WHEN dv.keywords ILIKE ${i+2} THEN 1 ELSE 0 END" 
                                    for i, _ in enumerate(keyword_list)])
            
            sql = f"""
                WITH keyword_analysis AS (
                    SELECT 
                        dv.chunk_text,
                        dv.metadata,
                        dv.keywords,
                        d.title as document_title,
                        d.uploaded_at,
                        
                        -- üìä –¢–û–ß–ù–´–ô –ü–û–î–°–ß–ï–¢ –°–û–í–ü–ê–î–ï–ù–ò–ô
                        ({total_matches}) as exact_matches_count,
                        
                        -- üìè –î–õ–ò–ù–ê –¢–ï–ö–°–¢–ê (–¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏)
                        LENGTH(dv.keywords) as keywords_length
                        
                    FROM document_vectors dv
                    JOIN documents d ON d.id = dv.document_id
                    WHERE dv.user_id = $1
                    AND ({where_clause})
                ),
                scored_chunks AS (
                    SELECT *,
                        -- üèÜ –£–õ–£–ß–®–ï–ù–ù–´–ô SCORE:
                        (
                            -- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π * 10 (–æ—Å–Ω–æ–≤–Ω–æ–π —Ñ–∞–∫—Ç–æ—Ä)
                            exact_matches_count * 10.0 +
                            
                            -- –ë–æ–Ω—É—Å –∑–∞ –ø–æ–ª–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≤—Å–µ—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
                            CASE WHEN exact_matches_count = {len(keyword_list)} THEN 5.0 ELSE 0.0 END +
                            
                            -- –ü–ª–æ—Ç–Ω–æ—Å—Ç—å –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
                            CASE WHEN keywords_length > 0 THEN 
                                (exact_matches_count::float / keywords_length * 100) * 2.0 
                            ELSE 0.0 END +
                            
                            -- –ë–æ–Ω—É—Å –∑–∞ –Ω–æ–≤–∏–∑–Ω—É –¥–æ–∫—É–º–µ–Ω—Ç–∞
                            CASE 
                                WHEN uploaded_at > NOW() - INTERVAL '7 days' THEN 3.0
                                WHEN uploaded_at > NOW() - INTERVAL '30 days' THEN 1.5
                                WHEN uploaded_at > NOW() - INTERVAL '90 days' THEN 0.5
                                ELSE 0.0
                            END
                        ) as advanced_score
                        
                    FROM keyword_analysis
                    WHERE exact_matches_count > 0
                )
                SELECT 
                    chunk_text,
                    metadata,
                    keywords,
                    document_title,
                    uploaded_at,
                    exact_matches_count,
                    advanced_score,
                    -- ‚úÖ –î–õ–Ø –°–û–í–ú–ï–°–¢–ò–ú–û–°–¢–ò —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –∫–æ–¥–æ–º:
                    advanced_score as rank,
                    exact_matches_count as matches_count
                FROM scored_chunks
                ORDER BY 
                    exact_matches_count DESC,      -- ü•á –°–Ω–∞—á–∞–ª–∞ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
                    advanced_score DESC,           -- ü•à –ü–æ—Ç–æ–º –ø–æ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–º—É score  
                    uploaded_at DESC               -- ü•â –ü–æ—Ç–æ–º –ø–æ –Ω–æ–≤–∏–∑–Ω–µ
                LIMIT {limit}
            """
            
            results = await conn.fetch(sql, *params)
            
            # üìä –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –∫–æ–¥–æ–º)
            chunks = []
            for row in results:
                try:
                    metadata = json.loads(row['metadata']) if row['metadata'] else {}
                except (json.JSONDecodeError, TypeError):
                    metadata = {}
                
                chunk_data = {
                    "chunk_text": row['chunk_text'],
                    "metadata": metadata,
                    "keywords": row['keywords'],
                    "document_title": row['document_title'],
                    "uploaded_at": row['uploaded_at'],
                    "rank": round(float(row['rank']), 3),                    # ‚úÖ –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å
                    "matches_count": int(row['matches_count']),              # ‚úÖ –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å
                    "exact_matches_count": int(row['exact_matches_count']),  # üÜï –ù–æ–≤–æ–µ –ø–æ–ª–µ
                    "advanced_score": round(float(row['advanced_score']), 3) # üÜï –ù–æ–≤–æ–µ –ø–æ–ª–µ
                }
                chunks.append(chunk_data)
            
            # üìà –£–ª—É—á—à–µ–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
            logger.info(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º")
            
            if chunks:
                print(f"\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –£–õ–£–ß–®–ï–ù–ù–û–ì–û –ö–õ–Æ–ß–ï–í–û–ì–û –ü–û–ò–°–ö–ê:")
                print(f"   üîë –ò—Å–∫–∞–ª–∏ —Å–ª–æ–≤–∞: {keyword_list}")
                for i, chunk in enumerate(chunks[:3]):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-3
                    matches = chunk['exact_matches_count']
                    score = chunk['advanced_score']
                    preview = chunk['chunk_text'][:50] + "..."
                    print(f"   {i+1}. ‚úÖ {matches}/{len(keyword_list)} —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π | Score: {score} | {preview}")
            
            return chunks
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º: {e}")
            return []
        finally:
            await self.db_pool.release(conn)
    
    async def delete_document_vectors(self, document_id: int):
        """–£–¥–∞–ª—è–µ—Ç –≤—Å–µ –≤–µ–∫—Ç–æ—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        conn = await self.db_pool.acquire()
        try:
            result = await conn.execute(
                "DELETE FROM document_vectors WHERE document_id = $1",
                document_id
            )
            deleted_count = int(result.split()[-1])  # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–¥–∞–ª–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–æ–∫
            logger.info(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–æ {deleted_count} –≤–µ–∫—Ç–æ—Ä–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞ {document_id}")
            return True
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞ {document_id}: {e}")
            return False
        finally:
            await self.db_pool.release(conn)
    
    async def delete_user_vectors(self, user_id: int):
        """–£–¥–∞–ª—è–µ—Ç –≤—Å–µ –≤–µ–∫—Ç–æ—Ä—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        conn = await self.db_pool.acquire()
        try:
            result = await conn.execute(
                "DELETE FROM document_vectors WHERE user_id = $1",
                user_id
            )
            deleted_count = int(result.split()[-1])
            logger.info(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–æ {deleted_count} –≤–µ–∫—Ç–æ—Ä–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}")
            return True
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}: {e}")
            return False
        finally:
            await self.db_pool.release(conn)
    
    async def get_vector_stats(self) -> Dict:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã"""
        conn = await self.db_pool.acquire()
        try:
            stats = await conn.fetchrow("""
                SELECT 
                    COUNT(*) as total_vectors,
                    COUNT(DISTINCT user_id) as unique_users,
                    COUNT(DISTINCT document_id) as unique_documents,
                    AVG(length(chunk_text)) as avg_chunk_length
                FROM document_vectors
            """)
            
            return {
                "total_vectors": stats['total_vectors'],
                "unique_users": stats['unique_users'], 
                "unique_documents": stats['unique_documents'],
                "avg_chunk_length": round(stats['avg_chunk_length'], 1) if stats['avg_chunk_length'] else 0
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
            return {"total_vectors": 0, "unique_users": 0, "unique_documents": 0}
        finally:
            await self.db_pool.release(conn)

# üåê –ì–õ–û–ë–ê–õ–¨–ù–´–ô –≠–ö–ó–ï–ú–ü–õ–Ø–† (–±—É–¥–µ—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –≤ main.py)
vector_db: Optional[PostgreSQLVectorDB] = None

async def search_similar_chunks(user_id: int, query: str, limit: int = 5) -> List[Dict]:
    """–ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö —á–∞–Ω–∫–æ–≤ (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å ChromaDB)"""
    if vector_db:
        return await vector_db.search_similar_chunks(user_id, query, limit)
    return []

async def keyword_search_chunks(user_id: int, keywords: str, limit: int = 5) -> List[Dict]:
    """–ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å ChromaDB)"""
    if vector_db:
        return await vector_db.keyword_search_chunks(user_id, keywords, limit)
    return []

async def extract_date_from_text(text: str) -> str:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞—Ç—É –∏–∑ —Ç–µ–∫—Å—Ç–∞ (–ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–æ –∏–∑ vector_utils.py)"""
    match = re.match(r"\[(\d{2})[./](\d{2})[./](\d{4})\]", text.strip())
    if match:
        try:
            date = datetime.strptime(".".join(match.groups()), "%d.%m.%Y")
            return date.strftime("%Y-%m-%d")
        except:
            pass
    return None

# ‚úÖ –û–ë–ù–û–í–õ–Ø–ï–ú —Ñ—É–Ω–∫—Ü–∏—é add_chunks_to_vector_db –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏:

async def add_chunks_to_vector_db(document_id: int, user_id: int, chunks: List[Dict]):
    """
    –î–æ–±–∞–≤–ª—è–µ—Ç —á–∞–Ω–∫–∏ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å ChromaDB)
    –¢–µ–ø–µ—Ä—å —ç—Ç–æ –æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è PostgreSQL —Ñ—É–Ω–∫—Ü–∏–∏
    """
    if vector_db:
        return await vector_db.add_document_chunks(document_id, user_id, chunks)
    return False

# ‚úÖ –î–û–ë–ê–í–õ–Ø–ï–ú —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –ø–æ–ª–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å vector_utils.py:

async def delete_all_chunks_by_user(user_id: int):
    """–£–¥–∞–ª—è–µ—Ç –≤—Å–µ –≤–µ–∫—Ç–æ—Ä—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å vector_db.py)"""
    if vector_db:
        return await vector_db.delete_user_vectors(user_id)
    return False

async def mark_chunks_unconfirmed(document_id: int):
    """
    –ü–æ–º–µ—á–∞–µ—Ç —á–∞–Ω–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∫–∞–∫ –Ω–µ–ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–Ω—ã–µ
    (–≤ PostgreSQL –≤–µ—Ä—Å–∏–∏ –º–æ–∂–Ω–æ –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤—ã–≤–∞—Ç—å –∏–ª–∏ —Å–¥–µ–ª–∞—Ç—å –∑–∞–≥–ª—É—à–∫—É)
    """
    # –í PostgreSQL –≤–µ—Ä—Å–∏–∏ —ç—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –º–æ–∂–µ—Ç –±—ã—Ç—å –∑–∞–≥–ª—É—à–∫–æ–π
    # —Ç–∞–∫ –∫–∞–∫ —É –Ω–∞—Å –Ω–µ—Ç –ø–æ–ª—è "confirmed" –∏–ª–∏ –æ–Ω–æ –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ
    logger.info(f"mark_chunks_unconfirmed({document_id}) - –∑–∞–≥–ª—É—à–∫–∞ –¥–ª—è PostgreSQL")
    return True

async def get_collection_stats():
    """–ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å vector_db.py)"""
    if vector_db:
        return await vector_db.get_vector_stats()
    return {"total_documents": 0, "status": "error"}

# ‚úÖ –§–£–ù–ö–¶–ò–ò –î–õ–Ø –†–ê–ë–û–¢–´ –° –≠–ú–ë–ï–î–î–ò–ù–ì–ê–ú–ò (–µ—Å–ª–∏ –Ω—É–∂–Ω—ã):

def validate_embedding_dimensions(embedding: List[float]) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞"""
    return len(embedding) == 1536  # OpenAI text-embedding-3-small

async def batch_get_embeddings(texts: List[str]) -> List[List[float]]:
    """–ü–æ–ª—É—á–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ (batch –æ–±—Ä–∞–±–æ—Ç–∫–∞)"""
    embeddings = []
    for text in texts:
        if vector_db:
            embedding = await vector_db.get_embedding(text)
            embeddings.append(embedding)
        else:
            embeddings.append([0.0] * 1536)  # –ó–∞–≥–ª—É—à–∫–∞
    return embeddings

# üåê –ì–õ–û–ë–ê–õ–¨–ù–´–ô –î–û–°–¢–£–ü –ö –ë–î –ü–£–õ–£
async def initialize_vector_db(db_pool=None):
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
    global vector_db
    
    # –ü–æ–ª—É—á–∞–µ–º –ø—É–ª –∏–∑ db_postgresql –µ—Å–ª–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω
    if db_pool is None:
        from db_postgresql import db_pool as main_db_pool
        db_pool = main_db_pool
    
    vector_db = PostgreSQLVectorDB(db_pool)
    await vector_db.initialize_vector_tables()
    logger.info("‚úÖ PostgreSQL Vector DB –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")

# üõ†Ô∏è –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø –í –°–£–©–ï–°–¢–í–£–Æ–©–ò–• –§–£–ù–ö–¶–ò–Ø–•

# –ò—Å–ø—Ä–∞–≤–ª—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é split_into_chunks –µ—Å–ª–∏ –µ—Å—Ç—å –ø—Ä–æ–±–ª–µ–º—ã —Å extract_keywords
async def split_into_chunks(summary: str, document_id: int, user_id: int) -> List[Dict]:
    """
    –†–∞–∑–±–∏–≤–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞ —á–∞–Ω–∫–∏ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
    –ü–µ—Ä–µ–Ω–µ—Å–µ–Ω–æ –∏–∑ vector_utils.py –∏ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è PostgreSQL
    """
    import tiktoken
    
    encoder = tiktoken.encoding_for_model("gpt-4")
    paragraphs = summary.strip().split("\n\n")
    now_str = datetime.now().strftime("%Y-%m-%d")

    chunks = []
    chunk_index = 0

    for para in paragraphs:
        clean_text = para.strip()
        if len(clean_text) < 20:
            continue

        token_count = len(encoder.encode(clean_text))
        
        found_date = await extract_date_from_text(clean_text)
        chunk_date = found_date if found_date else now_str

        # üîπ –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–±–µ–∑–æ–ø–∞—Å–Ω–æ)
        try:
            from gpt import extract_keywords
            keywords = await extract_keywords(clean_text)
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {e}")
            keywords = []  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –ø—Ä–∏ –æ—à–∏–±–∫–µ

        chunks.append({
            "chunk_text": clean_text,
            "chunk_index": chunk_index,
            "metadata": {
                "user_id": str(user_id),
                "document_id": str(document_id),
                "confirmed": 1,
                "source": "summary",
                "token_count": token_count,
                "created_at": chunk_date,
                "date_inside": found_date or "",
                "keywords": ", ".join(keywords) if keywords else ""
            }
        })
        chunk_index += 1
     

    return chunks

# üîß –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï –§–£–ù–ö–¶–ò–ò –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–ò

async def initialize_vector_db_safe():
    """–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã —Å –ø—Ä–æ–≤–µ—Ä–∫–∞–º–∏"""
    try:
        from db_postgresql import db_pool
        
        if db_pool is None:
            logger.error("‚ùå db_pool –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω!")
            return False
            
        await initialize_vector_db(db_pool)
        return True
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã: {e}")
        return False
    
def create_hybrid_ranking(vector_chunks: List[Dict], keyword_chunks: List[Dict], 
                         boost_factor: float = 1.8) -> List[str]:
    """
    üß† –ì–ò–ë–†–ò–î–ù–´–ô –ü–û–ò–°–ö —Å boost-—Ñ–∞–∫—Ç–æ—Ä–æ–º –¥–ª—è —á–∞–Ω–∫–æ–≤, –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –≤ –æ–±–æ–∏—Ö –ø–æ–∏—Å–∫–∞—Ö
    
    Args:
        vector_chunks: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
        keyword_chunks: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º  
        boost_factor: –ú–Ω–æ–∂–∏—Ç–µ–ª—å –¥–ª—è —á–∞–Ω–∫–æ–≤ –∏–∑ –æ–±–æ–∏—Ö –ø–æ–∏—Å–∫–æ–≤ (1.8 = +80%)
    
    Returns:
        –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ —á–∞–Ω–∫–æ–≤, –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ –≥–∏–±—Ä–∏–¥–Ω–æ–º—É score
    """
    
    chunk_scores = {}  # chunk_text -> score_data
    
    print(f"\nüîç –ì–ò–ë–†–ò–î–ù–û–ï –†–ê–ù–ñ–ò–†–û–í–ê–ù–ò–ï:")
    print(f"   üìä –í–µ–∫—Ç–æ—Ä–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(vector_chunks)}")
    print(f"   üîë –ö–ª—é—á–µ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(keyword_chunks)}")
    print(f"   ‚ö° Boost-—Ñ–∞–∫—Ç–æ—Ä: {boost_factor}")
    
    # ==========================================
    # –®–ê–ì 1: –û–ë–†–ê–ë–ê–¢–´–í–ê–ï–ú –í–ï–ö–¢–û–†–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´
    # ==========================================
    for i, chunk in enumerate(vector_chunks):
        chunk_text = chunk.get("chunk_text", "").strip()
        if not chunk_text:
            continue
            
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º similarity (0.0-1.0) –≤ score (0.0-10.0)
        vector_score = chunk.get("similarity", 0.0) * 10
        
        # –ë–æ–Ω—É—Å –∑–∞ –ø–æ–∑–∏—Ü–∏—é –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–º –ø–æ–∏—Å–∫–µ (—Ç–æ–ø —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–∞–∂–Ω–µ–µ)
        position_bonus = max(0, (len(vector_chunks) - i) * 0.1)
        
        chunk_scores[chunk_text] = {
            "vector_score": vector_score + position_bonus,
            "keyword_score": 0.0,
            "keyword_matches": 0,
            "found_in_vector": True,
            "found_in_keywords": False
        }
    
    # ==========================================
    # –®–ê–ì 2: –û–ë–†–ê–ë–ê–¢–´–í–ê–ï–ú –ö–õ–Æ–ß–ï–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´
    # ==========================================
    for i, chunk in enumerate(keyword_chunks):
        chunk_text = chunk.get("chunk_text", "").strip()
        if not chunk_text:
            continue
            
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π score –∏–∑ –Ω–æ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ keyword_search_chunks
        keyword_score = chunk.get("rank", 0.0)
        keyword_matches = chunk.get("matches_count", 0)
        
        # –ë–æ–Ω—É—Å –∑–∞ –ø–æ–∑–∏—Ü–∏—é –≤ –∫–ª—é—á–µ–≤–æ–º –ø–æ–∏—Å–∫–µ
        position_bonus = max(0, (len(keyword_chunks) - i) * 0.2)
        
        if chunk_text in chunk_scores:
            # üî• –ù–ê–ô–î–ï–ù –í –û–ë–û–ò–• –ü–û–ò–°–ö–ê–• - –ü–†–ò–ú–ï–ù–Ø–ï–ú BOOST!
            chunk_scores[chunk_text]["keyword_score"] = keyword_score + position_bonus
            chunk_scores[chunk_text]["keyword_matches"] = keyword_matches
            chunk_scores[chunk_text]["found_in_keywords"] = True
            print(f"   üî• BOOST: {keyword_matches} —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π | '{chunk_text[:40]}...'")
        else:
            # –ù–∞–π–¥–µ–Ω —Ç–æ–ª—å–∫–æ –≤ –∫–ª—é—á–µ–≤–æ–º –ø–æ–∏—Å–∫–µ
            chunk_scores[chunk_text] = {
                "vector_score": 0.0,
                "keyword_score": keyword_score + position_bonus,
                "keyword_matches": keyword_matches,
                "found_in_vector": False,
                "found_in_keywords": True
            }
    
    # ==========================================
    # –®–ê–ì 3: –í–´–ß–ò–°–õ–Ø–ï–ú –§–ò–ù–ê–õ–¨–ù–´–ï SCORES
    # ==========================================
    scored_chunks = []
    
    for chunk_text, data in chunk_scores.items():
        vector_score = data["vector_score"]
        keyword_score = data["keyword_score"] 
        keyword_matches = data["keyword_matches"]
        
        if data["found_in_vector"] and data["found_in_keywords"]:
            # üöÄ –ì–ò–ë–†–ò–î–ù–´–ô –†–ï–ó–£–õ–¨–¢–ê–¢ —Å boost
            base_score = (vector_score + keyword_score) / 2
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π boost –∑–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–≤–ø–∞–≤—à–∏—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            matches_multiplier = 1.0 + (keyword_matches * 0.15)  # +15% –∑–∞ –∫–∞–∂–¥–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
            
            final_score = base_score * boost_factor * matches_multiplier
            search_type = f"üî• HYBRID({keyword_matches})"
            
        elif data["found_in_vector"]:
            final_score = vector_score
            search_type = "üß† VECTOR"
        else:
            # –ö–ª—é—á–µ–≤–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å –±–æ–Ω—É—Å–æ–º –∑–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
            matches_multiplier = 1.0 + (keyword_matches * 0.1)
            final_score = keyword_score * matches_multiplier
            search_type = f"üîë KEYWORD({keyword_matches})"
        
        scored_chunks.append({
            "chunk_text": chunk_text,
            "final_score": final_score,
            "search_type": search_type,
            "keyword_matches": keyword_matches,
            "is_hybrid": data["found_in_vector"] and data["found_in_keywords"]
        })
    
    # ==========================================
    # –®–ê–ì 4: –°–û–†–¢–ò–†–û–í–ö–ê –ü–û –ü–†–ò–û–†–ò–¢–ï–¢–£
    # ==========================================
    def sort_key(item):
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: –≥–∏–±—Ä–∏–¥–Ω—ã–µ > –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π > —Ñ–∏–Ω–∞–ª—å–Ω—ã–π score
        return (item["is_hybrid"], item["keyword_matches"], item["final_score"])
    
    scored_chunks.sort(key=sort_key, reverse=True)
    
    # üìä –ü–æ–¥—Ä–æ–±–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
    hybrid_count = sum(1 for c in scored_chunks if c["is_hybrid"])
    print(f"\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ì–ò–ë–†–ò–î–ù–û–ì–û –†–ê–ù–ñ–ò–†–û–í–ê–ù–ò–Ø:")
    print(f"   üî• –ì–∏–±—Ä–∏–¥–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {hybrid_count}")
    print(f"   üìã –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö: {len(scored_chunks)}")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-5 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print(f"\nüèÜ –¢–û–ü-5 –†–ï–ó–£–õ–¨–¢–ê–¢–û–í:")
    for i, item in enumerate(scored_chunks[:5]):
        score = item["final_score"]
        search_type = item["search_type"]
        preview = item["chunk_text"][:50] + "..."
        print(f"   {i+1}. [{search_type}] Score: {score:.1f} | {preview}")
    
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç—ã —á–∞–Ω–∫–æ–≤ (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
    return [item["chunk_text"] for item in scored_chunks]